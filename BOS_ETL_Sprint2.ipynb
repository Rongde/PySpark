{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "Python_Glue_Session",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "pygments_lexer": "python3",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import length,col, explode, upper, to_date, date_sub, lag, coalesce, lit, array_sort, when, arrays_zip, size, date_format, explode_outer, from_json, concat, expr, array\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from operator import itemgetter\n",
    "import datetime, re, requests\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import concat, col, lit \n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import current_timestamp"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # read data from S3 to DataFrame\n",
    "# # input: S3\n",
    "# # output df['ROW_ID', 'BOS_FILE_EXTRACT', 'file_path', 'COLUMN_DEF']\n",
    "# def read_s3_to_df(day_path):\n",
    "#\n",
    "#     # parameter\n",
    "#     i = 0 # flag for initialize dataframe\n",
    "#\n",
    "#     # boto3 client\n",
    "#     s3 = boto3.client('s3')\n",
    "#     conn = client('s3')  # again assumes boto.cfg setup, assume AWS S3\n",
    "#\n",
    "#     # read data file by file\n",
    "#     for key in conn.list_objects(Bucket='bos-etl')['Contents']:\n",
    "#         path_key = key['Key']\n",
    "#         if path_key.endswith('cleansed'):\n",
    "#             if day_path in path_key:\n",
    "#                 file = s3.get_object(Bucket='bos-etl', Key=path_key)\n",
    "#                 txt = (file['Body'].read().decode('latin1'))\n",
    "#                 #st_re = txt.replace(\"\", \",\")\n",
    "#                 st_re_newline = txt.replace(\"!!! EOS !!!\", \"\\n\")\n",
    "#                 st_re_split = st_re_newline.split(\"\\n\")\n",
    "#                 df = pd.DataFrame(st_re_split)\n",
    "#                 df.index.name = 'ROW_ID'\n",
    "#                 df.rename({0:'BOS_FILE_EXTRACT'},axis='columns',inplace=True)\n",
    "#                 df[\"COLUMN_DEF\"]=df['BOS_FILE_EXTRACT'].replace(regex=r\"\\.*\",value=\"\")\n",
    "#                 rslt_dfRFT_temp = df[df['COLUMN_DEF'] =='PAX']\n",
    "#                 # rslt_dfRFT_temp = df\n",
    "#                 if ~rslt_dfRFT_temp.empty:\n",
    "#                     # print(path_key)\n",
    "#                     rslt_dfRFT_temp.insert(0,'file_path', path_key)\n",
    "#                     if (i ==0):\n",
    "#                         rslt_dfRFT = rslt_dfRFT_temp\n",
    "#                         i = 1\n",
    "#                     else:\n",
    "#                         rslt_dfRFT = pd.concat([rslt_dfRFT,rslt_dfRFT_temp])\n",
    "#     return rslt_dfRFT\n",
    "#\n",
    "#\n",
    "# # read data as DataFrame\n",
    "# #select day\n",
    "# day_path = 'date=2023-06-17'\n",
    "# # select current system day\n",
    "# # day_path = time.strftime('%Y-%m-%d')\n",
    "# rslt_dfRFT = read_s3_to_df(day_path)\n",
    "#\n",
    "# # test\n",
    "# #filter content\n",
    "# # check_list=['123','456']\n",
    "#\n",
    "# # for i in range():\n",
    "# #       rslt_dfRFT['BOS_FILE_EXTRACT'].filter(like=check_list[i])"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# #test write to s3 csv\n",
    "# from io import StringIO\n",
    "# bucket = 'bos-etl' # already created on S3\n",
    "# csv_buffer = StringIO()\n",
    "# rslt_dfRFT.to_csv(csv_buffer)\n",
    "# s3_resource = boto3.resource('s3')\n",
    "# s3_resource.Object(bucket, 'write_back/df.csv').put(Body=csv_buffer.getvalue())"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'ResponseMetadata': {'RequestId': 'E5XW3WFJZ77P4QQF', 'HostId': 'En2Wle7HGBj5tEq9b5hiR5QHRvYocV4ymHKqbazUpsJeUXjqoGfoKLTM/g8I+NQfDExdkFDbk00=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'En2Wle7HGBj5tEq9b5hiR5QHRvYocV4ymHKqbazUpsJeUXjqoGfoKLTM/g8I+NQfDExdkFDbk00=', 'x-amz-request-id': 'E5XW3WFJZ77P4QQF', 'date': 'Wed, 26 Jul 2023 19:41:06 GMT', 'x-amz-version-id': 'aOLY0w2HKoTQf5j3Ee_OxQwCwG8vGbmp', 'x-amz-server-side-encryption': 'AES256', 'etag': '\"e2b6a925db5723e142358c7a5fc0129d\"', 'server': 'AmazonS3', 'content-length': '0'}, 'RetryAttempts': 0}, 'ETag': '\"e2b6a925db5723e142358c7a5fc0129d\"', 'ServerSideEncryption': 'AES256', 'VersionId': 'aOLY0w2HKoTQf5j3Ee_OxQwCwG8vGbmp'}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# read csv\n",
    "rslt_dfRFT = pd.read_csv(\"source/input/1_df_RFT_PAT_PAX.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# import sys\n",
    "# from awsglue.transforms import *\n",
    "# from awsglue.utils import getResolvedOptions\n",
    "# from pyspark.context import SparkContext\n",
    "# from awsglue.context import GlueContext\n",
    "# from awsglue.job import Job\n",
    "#\n",
    "# ## @params: [JOB_NAME]\n",
    "# args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "#\n",
    "# sc = SparkContext()\n",
    "# glueContext = GlueContext(sc)\n",
    "# spark = glueContext.spark_session\n",
    "# job = Job(glueContext)\n",
    "# job.init(args['JOB_NAME'], args)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Pyspark Dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "#Create PySpark SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "#Create PySpark DataFrame from Pandas\n",
    "sparkDF=spark.createDataFrame(rslt_dfRFT)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/26 21:39:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Split Raw Data\n",
    "# input: spark dataframe\n",
    "# output: ['split','ITI', 'FAR','FOP_AND_FAR2','END','CER','EXC','split2','DCI','SAL','EXS','split3','split4']\n",
    "\n",
    "def split_raw_data(sparkDF):\n",
    "    data = sparkDF.withColumn(\"split\", F.split(\"BOS_FILE_EXTRACT\", \"<\")).withColumn(\"ITI\", F.element_at(\"split\", 2)).withColumn(\"FAR\", F.element_at(\"split\", 3))\\\n",
    "    .withColumn(\"FOP_AND_FAR2\", F.element_at(\"split\", 4)).withColumn(\"END\", F.element_at(\"split\", 5)).withColumn(\"CER\", F.element_at(\"split\", 6))\\\n",
    "    .withColumn(\"EXC\", F.element_at(\"split\", 7))\\\n",
    "    .withColumn(\"split2\", F.split(\"BOS_FILE_EXTRACT\", \"<\")).withColumn(\"DCI\", F.element_at(\"split2\", 1)).withColumn(\"SAL\", F.element_at(\"split2\", 2))\\\n",
    "    .withColumn(\"EXS\", F.element_at(\"split2\", -1))\\\n",
    "    .withColumn(\"split3\", F.split(\"BOS_FILE_EXTRACT\", \"<\")).withColumn(\"TAX2\", F.element_at(\"split3\", 2))\\\n",
    "    .withColumn(\"split4\", F.split(\"BOS_FILE_EXTRACT\", \"<\")).withColumn(\"TAX1\", F.element_at(\"split4\", 2))\\\n",
    "    .withColumn(\"SAL2\", F.element_at(\"split2\", 3))\n",
    "    return data\n",
    "\n",
    "df1 = split_raw_data(sparkDF)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(ROW_ID=45, file_path='data/date=2023-06-17/DCALLRECORDSAM.pgp.asc_3e5155823ea22a963da362086c693cf3_cleansed', BOS_FILE_EXTRACT='RFT\\x8022521623\\x8020230618\\x8023061805093326\\x80125\\x802161454822\\x804\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80<\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80<\\x82\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80<\\x83', COLUMN_DEF='RFT', split=['RFT\\x8022521623\\x8020230618\\x8023061805093326\\x80125\\x802161454822\\x804\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80<\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80<\\x82\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80<\\x83'], ITI=None, FAR=None, FOP_AND_FAR2=None, END=None, CER=None, EXC=None, split2=['RFT\\x8022521623\\x8020230618\\x8023061805093326\\x80125\\x802161454822\\x804\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80', '\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80', '\\x82\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80', '\\x83'], DCI='RFT\\x8022521623\\x8020230618\\x8023061805093326\\x80125\\x802161454822\\x804\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80', SAL='\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80', EXS='\\x83', split3=['RFT\\x8022521623\\x8020230618\\x8023061805093326\\x80125\\x802161454822\\x804\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80<\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80', '\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80<\\x83'], TAX2='\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80<\\x83', split4=['RFT\\x8022521623\\x8020230618\\x8023061805093326\\x80125\\x802161454822\\x804\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80<\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80<\\x82\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80<\\x83'], TAX1=None, SAL2='\\x82\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "split_df = df1.select(\"COLUMN_DEF\",\"DCI\",\"SAL\",\"TAX1\",\"TAX2\",\"ITI\",\"FAR\",\"FOP_AND_FAR2\",\"END\",\"CER\",\"EXC\",\"EXS\",split(df1.TAX1, '<').alias('split_text'),split(df1.FOP_AND_FAR2,'N<').alias(\"Splittext3\"),\"file_path\",\"SAL2\")\n",
    "split_df_type=split_df.selectExpr(\"column_def\",\"DCI\",\"SAL\",\"concat(split_text[0],',',TAX2) as TAX\",\"ITI\",\"FAR\", \"Case when column_def=='PAT' then FOP_AND_FAR2 else Splittext3[1] end as FOP_T\",\"Case when column_def=='PAT' then CER else END end as END_T\",\"Case when column_def=='PAT' then EXC else CER end as CER_T\",\"Case when column_def=='PAT' then '' else EXC end as EXC_T\",\"Case when column_def=='PAT' then '' else CER end as EXS_T\",\"file_path\",\"SAL2\")"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(COLUMN_DEF='RFT', DCI='RFT\\x8022521623\\x8020230618\\x8023061805093326\\x80125\\x802161454822\\x804\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80', SAL='\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80', TAX1=None, TAX2='\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80<\\x83', ITI=None, FAR=None, FOP_AND_FAR2=None, END=None, CER=None, EXC=None, EXS='\\x83', split_text=None, Splittext3=None, file_path='data/date=2023-06-17/DCALLRECORDSAM.pgp.asc_3e5155823ea22a963da362086c693cf3_cleansed', SAL2='\\x82\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80')]"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df.head(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # df2.write.options\n",
    "# split_df_type.coalesce(1).write.format('csv').option('header','true').mode(\"overwrite\").save(\"s3://bos-etl/write_back/\")"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# FTS - 2610  Commision: \"CAD\" - > \"USD\"\n",
    "# input: Dataframe\n",
    "# output: ['Transaction_Date','Source','Booking_channel','DCI_split','DCI','Agency_code',]\n",
    "\n",
    "def split_col(split_df_type):\n",
    "    data=split_df_type.withColumn(\"Transaction_Date\",current_timestamp()).withColumn(\"Source\",F.lit('BOS')).withColumn(\"Booking_channel\",F.lit('WEB')).withColumn(\"DCI_split\", F.split(\"DCI\", \"\")).withColumn(\"Agency_code\", F.element_at(\"DCI_split\", 2)).withColumn(\"Ticket_No\", concat(F.element_at(\"DCI_split\", 5),F.element_at(\"DCI_split\", 6))).withColumn(\"Issue_Date\", F.to_date(F.element_at(\"DCI_split\", 8),\"ddMMMyy\"))\\\n",
    "    .withColumn(\"SAL_split\", F.split(\"SAL\", \"\")).withColumn(\"PNR\", F.element_at(\"SAL_split\", 15)).withColumn(\"Tour_code\", F.regexp_replace(F.element_at(\"SAL_split\", 17),'','')).withColumn(\"Passenger\", F.element_at(\"SAL_split\", 16)).withColumn(\"Coupon_used\", F.element_at(\"SAL_split\", 7)).withColumn(\"Original_Fare\", F.element_at(\"SAL_split\", 9)).withColumn(\"Fare\", F.element_at(\"SAL_split\", 11)).withColumn(\"Org_currency\", F.element_at(\"SAL_split\", 10)).withColumn(\"Tax_Amt\", F.element_at(\"SAL_split\", 12)).withColumn(\"Total_amt\", F.element_at(\"SAL_split\", 8))\\\n",
    "    .withColumn(\"ITI_split\", F.split(\"ITI\", \"<\"))\\\n",
    "    .withColumn(\"Leg2\", F.element_at(\"ITI_split\", 13)).withColumn(\"Leg3\", F.element_at(\"ITI_split\", 22)).withColumn(\"Leg4\", F.element_at(\"ITI_split\", 31))\\\n",
    "    .withColumn(\"FOP_split\", F.split(\"FOP_T\", \"\"))\\\n",
    "    .withColumn(\"TAX_split\", F.split(\"tax\", \"<\"))\\\n",
    "    .withColumn(\"Commission\", concat(F.lit('[{\"type\":\"BASE\",\"amount\":'),F.element_at(\"SAL_split\", 13),f.lit(',\"currency\":\"USD\",\"commissionRate\":'),F.element_at(\"SAL_split\", 14),f.lit('}]')))\\\n",
    "    .withColumn(\"Fare_split\", F.split(\"FAR\", \"<\")).withColumn(\"FOP_split\", F.split(\"FOP_T\", \"<\")).withColumn(\"SAL2_split\", F.split(\"SAL2\", \"\")).withColumn(\"Passenger_rfnd\", F.element_at(\"SAL2_split\", 2))\\\n",
    "    .withColumn(\"EXC_split\", F.split(\"EXC_T\", \"\"))\n",
    "    \n",
    "    return data\n",
    "\n",
    "bos_df = split_col(split_df_type)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 112,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(column_def='RFT', DCI='RFT\\x8022521623\\x8020230618\\x8023061805093326\\x80125\\x802161454822\\x804\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80', SAL='\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80', TAX=None, ITI=None, FAR=None, FOP_T=None, END_T=None, CER_T=None, EXC_T=None, EXS_T=None, file_path='data/date=2023-06-17/DCALLRECORDSAM.pgp.asc_3e5155823ea22a963da362086c693cf3_cleansed', SAL2='\\x82\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80', Transaction_Date=datetime.datetime(2023, 7, 26, 23, 6, 12, 334267), Source='BOS', Booking_channel='WEB', DCI_split=['RFT', '22521623', '20230618', '23061805093326', '125', '2161454822', '4', '17JUN23', '', 'IAR', 'BA/D/ALLRECORDS/18JUN23', '103', ''], Agency_code='22521623', Ticket_No='1252161454822', Issue_Date=datetime.date(2023, 6, 17), SAL_split=['', '', '', '', '', '953.65', '0.00', '0.00', '358.00', '595.65', '0.00', '0.00', '0.00', '0.00', '', '', '\\x81125', '2161454822', '1234', ''], PNR='', Tour_code='125', Passenger='', Coupon_used='0.00', Original_Fare='358.00', Fare='0.00', Org_currency='595.65', Tax_Amt='0.00', Total_amt='0.00', ITI_split=None, Leg2=None, Leg3=None, Leg4=None, FOP_split=None, TAX_split=None, Commission='[{\"type\":\"BASE\",\"amount\":0.00,\"currency\":\"USD\",\"commissionRate\":0.00}]', Fare_split=None, SAL2_split=['\\x82', 'REDACTED REF NAME', 'F', '-953.65', '0.00', '0.00', '', '-953.65', 'CA', 'CASH', ''], Passenger_rfnd='REDACTED REF NAME', EXC_split=None)]"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_df.head(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### FTS - 2607\n",
    "bos_df_csv=bos_df.selectExpr(\"ITI_split\",\"column_def\",\"Transaction_Date\",\"Agency_code\",\"'06.07_DCALLRECORDSAM' as Filename\",\"Source\",\"Booking_channel\",\"case when column_def=='PAX' then 'EXCH-TKTT' when column_def=='PAT' then 'TKTT' else 'RFND' end as ticket_type\", \"1 as version_no\",\"Ticket_No\",\"Issue_date\",\"PNR\",\"Tour_code\",\"case when column_def=='RFT' then passenger_rfnd else Passenger end as Passenger_name\",\"1 as Passenger_Count\",\"Coupon_used\",\"'USA' AS Country_code\",\"Original_Fare\",\"case when Fare==0 then original_fare else fare end as Fare_amt\",\"Org_currency\",\"round((case when Fare==0 then original_fare else fare end)/Original_Fare,3) as exch_rate\",\"case when Fare==0 then original_fare else fare end as comm_amt\",\"Tax_Amt\",\"Total_amt\",\"'USD' as curr_code\",\"ticket_no as org_ticket_no\",\"Commission\",\"TAX_split\",\"FOP_split\",\"Fare_split\",\"file_path\",\"EXC_split\")"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 114,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------------+-----------+-------------+\n",
      "|ticket_type|      PNR|   Passenger_name|Coupon_used|org_ticket_no|\n",
      "+-----------+---------+-----------------+-----------+-------------+\n",
      "|       RFND|         |REDACTED REF NAME|       0.00|1252161454822|\n",
      "|  EXCH-TKTT|HNKNVK/AA|REDACTED SAL NAME|       FFVV|0017976703339|\n",
      "|       TKTT|   UOAGUL|REDACTED SAL NAME|       FFVV|0742100377105|\n",
      "+-----------+---------+-----------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bos_df_csv.select('ticket_type','PNR','Passenger_name',\"Coupon_used\",\"org_ticket_no\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# FTS - 2609\n",
    "def fc_Fare(li, column_def, org_ticket_no):\n",
    "    fare_v = ''\n",
    "    fare = ''\n",
    "    fare_end_v = ''\n",
    "    if column_def != 'RFT':\n",
    "        # print(row[\"Fare_split\"])\n",
    "\n",
    "        # li = row[\"Fare_split\"]\n",
    "        # print(li)\n",
    "        fare_v = ''\n",
    "        # print (fare_v)\n",
    "        if li is not None:\n",
    "            # FTS - 2609\n",
    "            # fare_end_v = []\n",
    "            fare_end_v = ''\n",
    "            fare_v = '['\n",
    "            i = 0\n",
    "            # print (fare_v)\n",
    "            for ii in li:\n",
    "                i = i + 1\n",
    "                ii_split = ii.split(\"\")\n",
    "\n",
    "                if i > 1:\n",
    "                    fare_v = fare_v + ',{\"sequence\":' + str(i) + ',\"content\":\"' + ii_split[0] + '\"}'\n",
    "                    fare = fare + ',' + ii_split[0]\n",
    "                else:\n",
    "                    fare_v = fare_v + '{\"sequence\":' + str(i) + ',\"content\":\"' + ii_split[0] + '\"}'\n",
    "                    fare = ii_split[0]\n",
    "                # print (fare_v)\n",
    "            fare_v = fare_v + ']'\n",
    "            # print(fare_v)\n",
    "        # FTS - 2609 spilt by \"END\" + \" \"\n",
    "        # fare_split = fare.split(\" \")\n",
    "        fare_split = fare.split(\"END\" + \" \")[0].split(\" \")\n",
    "        # print(fare_split)\n",
    "\n",
    "        for element in fare_split:\n",
    "            # FTS - 2609 add ~(element.startswith(\"Q\") & element[1].isdigit())\n",
    "            # if element.endswith(\"END\"):\n",
    "            if len(element) > 1:\n",
    "                if (False if (element.startswith(\"Q\") & element[1].isdigit()) else True):\n",
    "                    # FTS - 2609 list -> str\n",
    "                    fare_end_v = fare_end_v + element\n",
    "\n",
    "    return [fare_v, fare, fare_end_v]\n",
    "\n",
    "fc_Fare = udf(fc_Fare, ArrayType(StringType()))\n",
    "fc_Fare_list = fc_Fare('Fare_split', 'column_def')\n",
    "bos_df_csv = bos_df_csv.withColumn('Fare_Cons', fc_Fare_list[0])\\\n",
    "    .withColumn(\"Fare\", fc_Fare_list[1]).withColumn(\"Fare_end\",fc_Fare_list[2]).withColumn(\"Fare_split\", F.split(\"Fare\", \" \"))"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 116,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           Fare_Cons|            Fare_end|          Fare_split|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                    |                    |                  []|\n",
      "|[{\"sequence\":1,\"c...|ORFAAX/WASAAGRR16...|[ORF, AA, X/WAS, ...|\n",
      "|[{\"sequence\":1,\"c...|AMSKLMAD119.12KLA...|[AMS, KL, MAD119....|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bos_df_csv.select('Fare_Cons', \"Fare_end\", \"Fare_split\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# FTS - 2607\n",
    "# tax\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def fc(li, column_def):\n",
    "    tax_v = ''\n",
    "    if column_def != 'RFT':\n",
    "        tax_v =''\n",
    "        if li is not None:\n",
    "            #print(li)\n",
    "            tax_v ='['\n",
    "            i=0\n",
    "            for ii in li:\n",
    "                ii_split = ii.split(\",\")\n",
    "                # FTS 2607 Bug 2 & 3 : filter string which not start with digit\n",
    "                ii_split = [x for x in ii_split if x[0].isdigit()]\n",
    "                #print(ii_split)\n",
    "                for i1 in ii_split:\n",
    "                    i=i+1\n",
    "                    i1_split = i1.split(\"\")\n",
    "                    #print(i1_split)\n",
    "                    if i>1:\n",
    "                        # FTS-2607 Bug 1\n",
    "                        tax_v = tax_v+ ',{\"type\":\"'+ i1_split[1] +'\",\"amount\":'+i1_split[0] +',\"currency\":\"' +  \"Org_currency\" + '\"}'\n",
    "                    else:\n",
    "                        # FTS-2607 Bug 1\n",
    "                        tax_v = tax_v+ '{\"type\":\"'+ i1_split[1] +'\",\"amount\":'+i1_split[0] +',\"currency\":\"' +   \"Org_currency\" + '\"}'\n",
    "            tax_v=tax_v+']'\n",
    "    return tax_v\n",
    "\n",
    "fc = udf(fc, StringType())\n",
    "bos_df_csv = bos_df_csv.withColumn('Tax_v', fc('TAX_split', 'column_def'))\n",
    "bos_df_csv=bos_df_csv.selectExpr(\"ITI_split\",\"Fare_end\",\"column_def\",\"Transaction_Date\",\"Agency_code\",\"Filename\",\"Source\",\"Booking_channel\",\"ticket_type\", \"version_no\",\"Ticket_No\",\"Issue_date\",\"PNR\",\"Tour_code\",\"Passenger_name\",\"Passenger_Count\",\"Coupon_used\",\"Country_code\",\"Original_Fare\",\"Fare_amt\",\"Org_currency\",\"exch_rate\",\"comm_amt\",\"Tax_Amt\",\"Total_amt\",\"curr_code\",\"org_ticket_no\",\"Commission\",\"Fare_Cons\",\"case when column_def !='RFT' then replace(Tax_v,'Org_currency',Org_currency) end as Tax\",'FOP_split',\"file_path\",\"EXC_split\")"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 118,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|column_def|                 Tax|           EXC_split|\n",
      "+----------+--------------------+--------------------+\n",
      "|       RFT|                null|                null|\n",
      "|       PAX|[{\"type\":\"US\",\"am...|[001, 7976703339,...|\n",
      "|       PAT|[{\"type\":\"YR\",\"am...|                  []|\n",
      "+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bos_df_csv.select('column_def','Tax','EXC_split').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "bos_df_csv2=bos_df_csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# FTS - 2606\n",
    "# payment\n",
    "def fc_FOP(li, column_def,EXC_split, org_ticket_no):\n",
    "    print(column_def)\n",
    "    fop_v =''\n",
    "    if column_def != 'RFT':\n",
    "        # li = row[\"FOP_split\"]\n",
    "        if li is not None:\n",
    "            #print(li)\n",
    "            fop_v ='['\n",
    "            i=0\n",
    "            for ii in li:\n",
    "                ii_split = ii.split(\",\")\n",
    "                #print(ii_split)\n",
    "                for i1 in ii_split:\n",
    "                    i=i+1\n",
    "                    i1_split = i1.split(\"\") #FOP\n",
    "                    #print(i1_split)\n",
    "                    mode = i1_split[0][:2] if column_def == 'PAT' else 'EX'\n",
    "                    type1 = i1_split[0] if column_def == 'PAT' else 'EX'\n",
    "                    amount = i1_split[5] if column_def == 'PAT' else '0.00'\n",
    "                    accountNumber = i1_split[1] if column_def == 'PAT' else \\\n",
    "                        EXC_split[0] + EXC_split[1] + EXC_split[2] + EXC_split[18]\n",
    "                    org_ticket_no = org_ticket_no if column_def == 'PAT' else 'EX' + accountNumber[0:13]\n",
    "                    print(accountNumber)\n",
    "                    if i>1 :\n",
    "                        # FTS - 2606 \"mode\" add \",  mode <-> type\n",
    "                        fop_v = fop_v+ ',{\"mode\":\"'+ mode +'\",\"type\":\"'+ type1 + '\",\"amount\":'+ amount +',\"accountNumber\":\"'+accountNumber+ '\",\"approvalCode\":\"'+ i1_split[4]+'\",\"invoiceNumber\":\"\",\"invoiceNumber\":\"\",\"currency\":\"'+\"Org_currency\" + '\"}'\n",
    "                    else:\n",
    "                        # FTS - 2606 \"mode\" add \",  mode <-> type\n",
    "                        fop_v = fop_v+ '{\"mode\":\"'+ mode +'\",\"type\":\"'+ type1 +'\",\"amount\":'+ amount +',\"accountNumber\":\"'+accountNumber+ '\",\"approvalCode\":\"'+i1_split[4]+'\",\"invoiceNumber\":\"\",\"invoiceNumber\":\"\",\"currency\":\"'+\"Org_currency\" + '\"}'\n",
    "            fop_v=fop_v+']'\n",
    "\n",
    "    return [fop_v, org_ticket_no]\n",
    "\n",
    "fc_FOP = udf(fc_FOP, ArrayType(StringType()))\n",
    "fc_FOP_list = fc_FOP('FOP_split','column_def','EXC_split','org_ticket_no')\n",
    "bos_df_csv2 = bos_df_csv2.withColumn('Fop_v', fc_FOP_list[0]).withColumn('org_ticket_no', fc_FOP_list[1])\n",
    "bos_df_csv2=bos_df_csv2.selectExpr(\"ITI_split\",\"Fare_end\",\"column_def\",\"Transaction_Date\",\"Agency_code\",\"Filename\",\"Source\",\"Booking_channel\",\"ticket_type\", \"version_no\",\"Ticket_No\",\"Issue_date\",\"PNR\",\"Tour_code\",\"Passenger_name\",\"Passenger_Count\",\"Coupon_used\",\"Country_code\",\"Original_Fare\",\"Fare_amt\",\"Org_currency\",\"exch_rate\",\"comm_amt\",\"Tax_Amt\",\"Total_amt\",\"curr_code\",\"org_ticket_no\",\"Commission\",\"Fare_Cons\",\"Tax\",\"case when column_def !='RFT' then replace(Fop_v,'Org_currency',Org_currency) end as FOP\",\"file_path\")"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 131,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PATPAX\n",
      "00179767033390REDACTED EXC NAME\n",
      "\n",
      "CASH\n",
      "RFT\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(FOP=None, Commission='[{\"type\":\"BASE\",\"amount\":0.00,\"currency\":\"USD\",\"commissionRate\":0.00}]', org_ticket_no='1252161454822'),\n Row(FOP='[{\"mode\":\"EX\",\"type\":\"EX\",\"amount\":0.00,\"accountNumber\":\"00179767033390REDACTED EXC NAME\",\"approvalCode\":\"\",\"invoiceNumber\":\"\",\"invoiceNumber\":\"\",\"currency\":\"USD\"}]', Commission='[{\"type\":\"BASE\",\"amount\":1.00,\"currency\":\"USD\",\"commissionRate\":0.60}]', org_ticket_no='EX0017976703339'),\n Row(FOP='[{\"mode\":\"CA\",\"type\":\"CA\",\"amount\":302.20,\"accountNumber\":\"CASH\",\"approvalCode\":\"\",\"invoiceNumber\":\"\",\"invoiceNumber\":\"\",\"currency\":\"USD\"}]', Commission='[{\"type\":\"BASE\",\"amount\":0.00,\"currency\":\"USD\",\"commissionRate\":0.00}]', org_ticket_no='0742100377105')]"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_df_csv2.select('FOP','Commission','org_ticket_no').head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "# FTS - 2608 rewrtie function\n",
    "# FTS - 2608 \"currency\": \"CAD\" -> \"currency\": \"USD\"\n",
    "def fc_LEG(li, fare_row, ticket_no, issue_date, column_def):\n",
    "    leg_v =''\n",
    "    if column_def != 'RFT':\n",
    "        #print(row)\n",
    "        # li = row[\"ITI_split\"]\n",
    "        # fare_row = row[\"Fare_end\"]\n",
    "        issue_year = issue_date.year\n",
    "        issue_date = issue_date.strftime('%Y-%m-%d')\n",
    "        leap_year = '2024' # not use this value just avoid error from strptime\n",
    "        if li is not None:\n",
    "            #print(li)\n",
    "            leg_v ='[' # segment 1\n",
    "            leg_v_1 = '[' #segment 2\n",
    "            # i=0\n",
    "            # FTS - 2608\n",
    "            i = 0\n",
    "            j = 1 # index of fare , max len 2\n",
    "            fareval = []\n",
    "            if (len(li) > 1): # if two segment, reverse, direction right to left\n",
    "                li.reverse()\n",
    "                i = 3\n",
    "            for ii in li:\n",
    "                ii_split = ii.split(\",\")\n",
    "                #print(ii_split)\n",
    "                for i1 in ii_split:\n",
    "                    fareval = re.findall(r'\\d+.\\d+', str(fare_row))\n",
    "                    if len(fareval)==0:\n",
    "                        total_fareval = '0.00'\n",
    "                        fareval=['0.00','0.00','0.00','0.00']\n",
    "                    else:\n",
    "                        total_fareval = fareval[-1]\n",
    "                        fareval = fareval[:-1]\n",
    "                    #print(fareval)\n",
    "                    i = i - 1 # reverse\n",
    "                    i1_split = i1.split(\"\")\n",
    "                    if len(i1_split[0])!=0:\n",
    "                        conj= ticket_no[0:3] + i1_split[0]\n",
    "                    else:\n",
    "                        conj='CONJ'\n",
    "\n",
    "                    if i>1:\n",
    "                       if len(i1_split[30])!=0:\n",
    "                            if len(i1_split[35])!=0:\n",
    "                                dep_day = datetime.strptime(i1_split[35] + leap_year,'%d%b%Y').strftime(\"%m-%d\")\n",
    "                                dep_year = str(issue_year) if dep_day >= issue_date[5:] else str(int(issue_year)    + 1)\n",
    "                                dep_on = pd.datetime.strptime(i1_split[35]+ dep_year,'%d%b%Y')\n",
    "                            else:\n",
    "                                dep_on= 'NULL'\n",
    "                            if i1_split[29]=='X':\n",
    "                                fare = str('0.00')\n",
    "                            else:\n",
    "                                if (len(fareval) == 0):\n",
    "                                    fare ='NULL'\n",
    "                                elif (j <= len(fareval)):\n",
    "                                    fare = fareval[-j]\n",
    "                                    j = j + 1\n",
    "                                else:\n",
    "                                    fare = 'NULL'\n",
    "                                    j = j + 1\n",
    "                            leg_v_1 = ',{\"departure\":\"'+ i1_split[30] +'\",\"destination\":\"'+i1_split[31]     +'\",\"seatClass\":\"' +  i1_split[34] +'\",\"conjunction\":\"'+  conj  +'\",\"carrier\":\"'+i1_split[32]+'\",\"tripCode\":\"'+i1_split[33]+'\",\"departureOn\":\"'+str( dep_on)+'\",\"designator\":\"'+i1_split[37]+'\",\"stopOver\":\"'+i1_split[   29]+'\",\"flyerCode\":\"\",\"fare\":'+ fare + ',\"currency\": \"USD\",\"originalFare\":'+ fare  +',\"originalCurrency\":\"'+'org_currency'+'\"}'\n",
    "\n",
    "                       if len(i1_split[21])!=0:\n",
    "                            if len(i1_split[26])!=0:\n",
    "                                dep_day = datetime.strptime(i1_split[26] + leap_year,'%d%b%Y').strftime(\"%m-%d\")\n",
    "                                dep_year = str(issue_year) if dep_day >= issue_date[5:] else str(int(issue_year)    + 1)\n",
    "                                dep_on = pd.datetime.strptime(i1_split[26]+ dep_year,'%d%b%Y')\n",
    "                            else:\n",
    "                                dep_on= 'NULL'\n",
    "                            if i1_split[20]=='X':\n",
    "                                fare = str('0.00')\n",
    "                            else:\n",
    "                                if (len(fareval) == 0):\n",
    "                                    fare ='NULL'\n",
    "                                elif (j <= len(fareval)):\n",
    "                                    fare = fareval[-j]\n",
    "                                    j = j + 1\n",
    "                                else:\n",
    "                                    fare = 'NULL'\n",
    "                                    j = j + 1\n",
    "                            leg_v_1 = '' if leg_v_1 == '[' else leg_v_1\n",
    "                            leg_v_1 = ',{\"departure\":\"'+ i1_split[21] +'\",\"destination\":\"'+i1_split[22]     +'\",\"seatClass\":\"' + i1_split[25] +'\",\"conjunction\":\"'+  conj   +'\",\"carrier\":\"'+i1_split[23]+'\",\"tripCode\":\"'+i1_split[24]+'\",\"departureOn\":\"'+str(  dep_on)+'\",\"designator\":\"'+i1_split[28]+'\",\"stopOver\":\"'+i1_split[    20]+'\",\"flyerCode\":\"\",\"fare\":'+ fare +',\"currency\": \"USD\",\"originalFare\":'+ fare    +',\"originalCurrency\":\"'+'org_currency'+'\"}' + leg_v_1\n",
    "\n",
    "                       if len(i1_split[12])!=0:\n",
    "                            if len(i1_split[17])!=0:\n",
    "                                dep_day = datetime.strptime(i1_split[17] + leap_year,'%d%b%Y').strftime(\"%m-%d\")\n",
    "                                dep_year = str(issue_year) if dep_day >= issue_date[5:] else str(int(issue_year)    + 1)\n",
    "                                dep_on = pd.datetime.strptime(i1_split[17]+ dep_year,'%d%b%Y')\n",
    "                            else:\n",
    "                                dep_on= 'NULL'\n",
    "                            if i1_split[11]=='X':\n",
    "                                fare = str('0.00')\n",
    "                            else:\n",
    "                                if (len(fareval) == 0):\n",
    "                                    fare ='NULL'\n",
    "                                elif (j <= len(fareval)):\n",
    "                                    fare = fareval[-j]\n",
    "                                    j = j + 1\n",
    "                                else:\n",
    "                                    fare = 'NULL'\n",
    "                                    j = j + 1\n",
    "                            leg_v_1 = '' if leg_v_1 == '[' else leg_v_1\n",
    "                            leg_v_1 = ',{\"departure\":\"'+ i1_split[12] +'\",\"destination\":\"'+i1_split[13]     +'\",\"seatClass\":\"' + i1_split[16] +'\",\"conjunction\":\"'+  conj   +'\",\"carrier\":\"'+i1_split[14]+'\",\"tripCode\":\"'+i1_split[15]+'\",\"departureOn\":\"'+str(  dep_on)+'\",\"designator\":\"'+i1_split[19]+'\",\"stopOver\":\"'+i1_split[    11]+'\",\"flyerCode\":\"\",\"fare\":'+ fare +',\"currency\": \"USD\",\"originalFare\":'+ fare    +',\"originalCurrency\":\"'+'org_currency'+'\"}' + leg_v_1\n",
    "\n",
    "                       if len(i1_split[3])!=0:\n",
    "                            if len(i1_split[8])!=0:\n",
    "                                dep_day = datetime.strptime(i1_split[8] + leap_year,'%d%b%Y').strftime(\"%m-%d\")\n",
    "                                dep_year = str(issue_year) if dep_day >= issue_date[5:] else str(int(issue_year)    + 1)\n",
    "                                dep_on = pd.datetime.strptime(i1_split[8]+ dep_year,'%d%b%Y')\n",
    "                            else:\n",
    "                                dep_on= 'NULL'\n",
    "                            if i1_split[2]=='X':\n",
    "                                fare = str('0.00')\n",
    "                            else:\n",
    "                                if (len(fareval) == 0):\n",
    "                                    fare ='NULL'\n",
    "                                elif (j <= len(fareval)):\n",
    "                                    fare = fareval[-j]\n",
    "                                    j = j + 1\n",
    "                                else:\n",
    "                                    fare = 'NULL'\n",
    "                                    j = j + 1\n",
    "\n",
    "                            leg_v_1 = '' if leg_v_1 == '[' else leg_v_1\n",
    "                            leg_v_1 = ',{\"departure\":\"'+ i1_split[3] +'\",\"destination\":\"'+i1_split[4]   +'\",\"seatClass\":\"' + i1_split[7] +'\",\"conjunction\":\"'+  conj +'\",\"carrier\":\"'+i1_split    [5]+'\",\"tripCode\":\"'+i1_split[6]+'\",\"departureOn\":\"'+str(   dep_on)+'\",\"designator\":\"'+i1_split[10]+'\",\"stopOver\":\"'+i1_split[ 2]+'\",\"flyerCode\":\"\",\"fare\":'+ fare +',\"currency\": \"USD\",\"originalFare\":'+ fare  +',\"originalCurrency\":\"'+'org_currency'+'\"}' + leg_v_1\n",
    "\n",
    "                    else:\n",
    "                        if len(i1_split[30])!=0:\n",
    "                            if len(i1_split[35])!=0:\n",
    "                                dep_day = datetime.strptime(i1_split[35] + leap_year,'%d%b%Y').strftime(\"%m-%d\")\n",
    "                                dep_year = str(issue_year) if dep_day >= issue_date[5:] else str(int(issue_year)    + 1)\n",
    "                                dep_on = pd.datetime.strptime(i1_split[35]+ dep_year,'%d%b%Y')\n",
    "                            else:\n",
    "                                dep_on= 'NULL'\n",
    "                            if i1_split[29]=='X':\n",
    "                                fare = str('0.00')\n",
    "                            else:\n",
    "                                if (len(fareval) == 0):\n",
    "                                    fare ='NULL'\n",
    "                                elif (j <= len(fareval)):\n",
    "                                    fare = fareval[-j]\n",
    "                                    j = j + 1\n",
    "                                else:\n",
    "                                    fare = 'NULL'\n",
    "                                    j = j + 1\n",
    "\n",
    "                            leg_v = ',{\"departure\":\"'+ i1_split[30] +'\",\"destination\":\"'+i1_split[31]   +'\",\"seatClass\":\"' + i1_split[34] +'\",\"conjunction\":\"'+  conj     +'\",\"carrier\":\"'+i1_split[32]+'\",\"tripCode\":\"'+i1_split[33]+'\",\"departureOn\":\"'+str(    dep_on)+'\",\"designator\":\"'+i1_split[37]+'\",\"stopOver\":\"'+i1_split[  29]+'\",\"flyerCode\":\"\",\"fare\":'+ fare + ',\"currency\": \"USD\",\"originalFare\":'+ fare     +',\"originalCurrency\":\"'+'org_currency'+'\"}'\n",
    "\n",
    "                        if len(i1_split[21])!=0:\n",
    "                            if len(i1_split[26])!=0:\n",
    "                                dep_day = datetime.strptime(i1_split[26] + leap_year,'%d%b%Y').strftime(\"%m-%d\")\n",
    "                                dep_year = str(issue_year) if dep_day >= issue_date[5:] else str(int(issue_year)    + 1)\n",
    "                                dep_on = pd.datetime.strptime(i1_split[26]+ dep_year,'%d%b%Y')\n",
    "                            else:\n",
    "                                dep_on= 'NULL'\n",
    "                            if i1_split[20]=='X':\n",
    "                                fare = str('0.00')\n",
    "                            else:\n",
    "                                if (len(fareval) == 0):\n",
    "                                    fare ='NULL'\n",
    "                                elif (j <= len(fareval)):\n",
    "                                    fare = fareval[-j]\n",
    "                                    j = j + 1\n",
    "                                else:\n",
    "                                    fare = 'NULL'\n",
    "                                    j = j + 1\n",
    "                            leg_v = '' if leg_v == '[' else leg_v\n",
    "                            leg_v =  ',{\"departure\":\"'+ i1_split[21] +'\",\"destination\":\"'+i1_split[22]  +'\",\"seatClass\":\"' + i1_split[25] +'\",\"conjunction\":\"'+  conj    +'\",\"carrier\":\"'+i1_split[23]+'\",\"tripCode\":\"'+i1_split[24]+'\",\"departureOn\":\"'+str(   dep_on)+'\",\"designator\":\"'+i1_split[28]+'\",\"stopOver\":\"'+i1_split[ 20]+'\",\"flyerCode\":\"\",\"fare\":'+ fare +',\"currency\": \"USD\",\"originalFare\":'+ fare     +',\"originalCurrency\":\"'+'org_currency'+'\"}' + leg_v\n",
    "\n",
    "                        if len(i1_split[12])!=0:\n",
    "                            if len(i1_split[17])!=0:\n",
    "                                dep_day = datetime.strptime(i1_split[17] + leap_year,'%d%b%Y').strftime(\"%m-%d\")\n",
    "                                dep_year = str(issue_year) if dep_day >= issue_date[5:] else str(int(issue_year)    + 1)\n",
    "                                dep_on = pd.datetime.strptime(i1_split[17]+ dep_year,'%d%b%Y')\n",
    "                            else:\n",
    "                                dep_on= 'NULL'\n",
    "                            if i1_split[11]=='X':\n",
    "                                fare = str('0.00')\n",
    "                            else:\n",
    "                                if (len(fareval) == 0):\n",
    "                                    fare ='NULL'\n",
    "                                elif (j <= len(fareval)):\n",
    "                                    fare = fareval[-j]\n",
    "                                    j = j + 1\n",
    "                                else:\n",
    "                                    fare = 'NULL'\n",
    "                                    j = j + 1\n",
    "                            leg_v = '' if leg_v == '[' else leg_v\n",
    "                            leg_v =   ',{\"departure\":\"'+ i1_split[12] +'\",\"destination\":\"'+i1_split[13]     +'\",\"seatClass\":\"' + i1_split[16] +'\",\"conjunction\":\"'+  conj   +'\",\"carrier\":\"'+i1_split[14]+'\",\"tripCode\":\"'+i1_split[15]+'\",\"departureOn\":\"'+str(  dep_on)+'\",\"designator\":\"'+i1_split[19]+'\",\"stopOver\":\"'+i1_split[    11]+'\",\"flyerCode\":\"\",\"fare\":'+ fare +',\"currency\": \"USD\",\"originalFare\":'+ fare    +',\"originalCurrency\":\"'+'org_currency'+'\"}' + leg_v\n",
    "\n",
    "                        if len(i1_split[3])!=0:\n",
    "                            #print(conj)\n",
    "                            if len(i1_split[8])!=0:\n",
    "                                dep_day = datetime.strptime(i1_split[8] + leap_year,'%d%b%Y').strftime(\"%m-%d\")\n",
    "                                dep_year = str(issue_year) if dep_day >= issue_date[5:] else str(int(issue_year)    + 1)\n",
    "                                dep_on = pd.datetime.strptime(i1_split[8]+ dep_year,'%d%b%Y')\n",
    "                            else:\n",
    "                                dep_on= 'NULL'\n",
    "                            if i1_split[2]=='X':\n",
    "                                fare = str('0.00')\n",
    "                            else:\n",
    "                                if (len(fareval) == 0):\n",
    "                                    fare ='NULL'\n",
    "                                elif (j <= len(fareval)):\n",
    "                                    fare = fareval[-j]\n",
    "                                    j = j + 1\n",
    "                                else:\n",
    "                                    fare = 'NULL'\n",
    "                                    j = j + 1\n",
    "                            leg_v = '' if leg_v == '[' else leg_v\n",
    "                            leg_v = '[{\"departure\":\"'+ i1_split[3] +'\",\"destination\":\"'+i1_split[4]     +'\",\"seatClass\":\"' + i1_split[7] +'\",\"conjunction\":\"'+  conj +'\",\"carrier\":\"'+i1_split  [5]+'\",\"tripCode\":\"'+i1_split[6]+'\",\"departureOn\":\"'+str( dep_on)+'\",\"designator\":\"'+i1_split[10]+'\",\"stopOver\":\"'+i1_split[   2]+'\",\"flyerCode\":\"\",\"fare\":'+ fare +',\"currency\": \"USD\",\"originalFare\":'+ fare    +',\"originalCurrency\":\"'+'org_currency'+'\"}' + leg_v\n",
    "\n",
    "                        # segment 1 + segment2\n",
    "                        leg_v_1 = '' if leg_v_1 == '[' else leg_v_1\n",
    "                        leg_v = leg_v + leg_v_1\n",
    "                    #print(pd.datetime.strptime(i1_split[8]+ str(datetime.now().year),'%d%b%Y'))\n",
    "            leg_v=leg_v+']'\n",
    "\n",
    "            # attributes: len(leg) > len(fare)\n",
    "            attributes = '' if j == len(fareval) + 1 else ' # ' + 'FareLegMismatch:  leg: ' + str(j-1) +  ' does    not match fare: ' + str(len(fareval)) + ' # '\n",
    "\n",
    "            # atributes: total_fareval <> leg: fareval\n",
    "            leg_fer_sum = '%.2f'%sum([float(x[7:]) for x in re.findall(r'\\\"fare\\\"\\:\\d+\\.\\d+', leg_v)])\n",
    "            attributes = attributes if total_fareval == leg_fer_sum else '#' + 'FareAmountMismatch: Leg sum     amount: ' + str(leg_fer_sum) + ' does not match fare amount: ' + str(total_fareval)  + ' # '\n",
    "\n",
    "    return [leg_v, attributes]\n",
    "\n",
    "fc_LEG = udf(fc_LEG, ArrayType(StringType()))\n",
    "fc_LEG_list = fc_LEG('ITI_split','Fare_end','Ticket_no','Issue_date','column_def')\n",
    "bos_df_csv = bos_df_csv.withColumn('Leg_v1', fc_LEG_list[0]).withColumn('attributes', fc_LEG_list[1])\n",
    "bos_df_csv=bos_df_csv.selectExpr(\"FOP\",\"column_def\",\"Transaction_Date\",\"Agency_code\",\"Filename\",\"Source\",\"Booking_channel\",\"ticket_type\", \"version_no\",\"Ticket_No\",\"Issue_date\",\"PNR\",\"Tour_code\",\"Passenger_name\",\"Passenger_Count\",\"Coupon_used\",\"Country_code\",\"Original_Fare\",\"Fare_amt\",\"Org_currency\",\"exch_rate\",\"comm_amt\",\"Tax_Amt\",\"Total_amt\",\"curr_code\",\"org_ticket_no\",\"Commission\",\"Fare_Cons\",\"Tax\",\"case when column_def !='RFT' then replace(replace(Leg_v1,'CONJ',Ticket_no),'org_currency',Org_currency) else '[]' end as Legs\",\"attributes\",\"file_path\")"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/26 21:21:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/07/26 21:21:32 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 17)   \n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/d9/871p0v1d2cn4c6y7bj_xghhh0000gn/T/ipykernel_2618/134353313.py\", line 31, in fc_Fare\n",
      "UnboundLocalError: local variable 'fare' referenced before assignment\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:57)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/07/26 21:21:32 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 17) (rongs-mbp.ht.home executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/d9/871p0v1d2cn4c6y7bj_xghhh0000gn/T/ipykernel_2618/134353313.py\", line 31, in fc_Fare\n",
      "UnboundLocalError: local variable 'fare' referenced before assignment\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:57)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/07/26 21:21:32 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job\n",
      "23/07/26 21:21:32 WARN TaskSetManager: Lost task 2.0 in stage 9.0 (TID 19) (rongs-mbp.ht.home executor driver): TaskKilled (Stage cancelled)\n",
      "/var/folders/d9/871p0v1d2cn4c6y7bj_xghhh0000gn/T/ipykernel_2618/2710465565.py:176: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "/var/folders/d9/871p0v1d2cn4c6y7bj_xghhh0000gn/T/ipykernel_2618/2710465565.py:198: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "23/07/26 21:21:33 WARN TaskSetManager: Lost task 1.0 in stage 9.0 (TID 18) (rongs-mbp.ht.home executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/d9/871p0v1d2cn4c6y7bj_xghhh0000gn/T/ipykernel_2618/134353313.py\", line 31, in fc_Fare\nUnboundLocalError: local variable 'fare' referenced before assignment\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbos_df_csv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhead\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/sql/dataframe.py:2823\u001B[0m, in \u001B[0;36mDataFrame.head\u001B[0;34m(self, n)\u001B[0m\n\u001B[1;32m   2821\u001B[0m     rs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   2822\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m rs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m rs \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2823\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/sql/dataframe.py:1322\u001B[0m, in \u001B[0;36mDataFrame.take\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtake\u001B[39m(\u001B[38;5;28mself\u001B[39m, num: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Row]:\n\u001B[1;32m   1294\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001B[39;00m\n\u001B[1;32m   1295\u001B[0m \n\u001B[1;32m   1296\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1320\u001B[0m \u001B[38;5;124;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\u001B[39;00m\n\u001B[1;32m   1321\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/sql/dataframe.py:1216\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1196\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[1;32m   1197\u001B[0m \n\u001B[1;32m   1198\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001B[39;00m\n\u001B[1;32m   1214\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1215\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[0;32m-> 1216\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:175\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    171\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mPythonException\u001B[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/d9/871p0v1d2cn4c6y7bj_xghhh0000gn/T/ipykernel_2618/134353313.py\", line 31, in fc_Fare\nUnboundLocalError: local variable 'fare' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "bos_df_csv.head(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Data Validation\n",
    "# def\n",
    "def check_float(data, msg, header, length, decimal = 0):\n",
    "    try:\n",
    "        data_1 = data\n",
    "        data = data if len(str(data).split('.')[0]) <= (length - decimal) else 'NULL'\n",
    "        msg = msg if str(data) == str(data_1) else msg + ' # ' + header + 'OutOfRange: ' + str(data_1) + 'is out of range of (' +  str(length) + ',' + str(decimal) + ') # '\n",
    "    except Exception as e:\n",
    "        data = 'NULL'\n",
    "        msg = msg + '# ERROR ' + str(e) + ' # '\n",
    "    return data, msg\n",
    "\n",
    "\n",
    "def fc_DV(attributes,original_fare, exchange_rate,fare_amount,tax_amount,total_amount):\n",
    "\n",
    "    # attributes\n",
    "    attributes = '' if attributes is None else attributes\n",
    "\n",
    "    # original_fare\n",
    "    # numeric(12, 5)\n",
    "    original_fare, attributes = check_float(data = original_fare, msg = attributes , header = 'original_fare', length = 12, decimal = 5)\n",
    "\n",
    "    # exchange_rate\n",
    "    # numeric(12, 5)\n",
    "    exchange_rate, attributes = check_float(data = exchange_rate, msg = attributes , header = 'exchange_rate', length = 12, decimal = 5)\n",
    "\n",
    "    # fare_amount\n",
    "    # numeric(12, 5)\n",
    "    fare_amount, attributes = check_float(data = fare_amount, msg = attributes , header = 'fare_amount', length = 12, decimal = 5)\n",
    "\n",
    "    # tax_amount\n",
    "    # numeric(12, 5)\n",
    "    tax_amount, attributes = check_float(data = tax_amount, msg = attributes , header = 'tax_amount', length = 12, decimal = 5)\n",
    "\n",
    "    # total_amount\n",
    "    # numeric(12, 5)\n",
    "    total_amount, attributes = check_float(data = total_amount, msg = attributes , header = 'total_amount', length = 12, decimal = 5)\n",
    "\n",
    "    return [attributes,original_fare,exchange_rate,fare_amount,tax_amount,total_amount]\n",
    "\n",
    "fc_DV = udf(fc_DV, ArrayType(StringType()))\n",
    "fc_DV_list = fc_DV('attributes', 'Original_Fare', 'exch_rate', 'Fare_amt', 'Tax_Amt', 'Total_amt')\n",
    "bos_df_csv = bos_df_csv\\\n",
    "    .withColumn('attributes', fc_DV_list[0])\\\n",
    "    .withColumn('Original_Fare', fc_DV_list[1])\\\n",
    "    .withColumn('exch_rate', fc_DV_list[2])\\\n",
    "    .withColumn('Fare_amt', fc_DV_list[3])\\\n",
    "    .withColumn('Tax_Amt', fc_DV_list[4])\\\n",
    "    .withColumn('Total_amt', fc_DV_list[5])"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def fc_DV_2(attributes, Agency_code,Source,ticket_type,Coupon_used):\n",
    "\n",
    "    # attributes\n",
    "    attributes = '' if attributes is None else attributes\n",
    "\n",
    "    # Agency_code\n",
    "    # numeric(10, 0)\n",
    "    Agency_code, attributes = check_float(data = Agency_code, msg = attributes , header = 'Agency_code', length = 10, decimal = 0)\n",
    "\n",
    "    # Source\n",
    "    # numeric(10, 0)\n",
    "    Source, attributes = check_float(data = Source, msg = attributes , header = 'Source', length = 10, decimal = 0)\n",
    "\n",
    "    # ticket_type\n",
    "    # numeric(10, 0)\n",
    "    ticket_type, attributes = check_float(data = ticket_type, msg = attributes , header = 'ticket_type', length = 10, decimal = 0)\n",
    "\n",
    "\n",
    "    # Coupon_used\n",
    "    # numeric(10, 0)\n",
    "    Coupon_used, attributes = check_float(data = Coupon_used, msg = attributes , header = 'Coupon_used', length = 10, decimal = 0)\n",
    "\n",
    "\n",
    "    return [attributes,Agency_code,Source,ticket_type,Coupon_used]\n",
    "\n",
    "fc_DV_2 = udf(fc_DV_2, ArrayType(StringType()))\n",
    "fc_DV_2_list = fc_DV_2('attributes', 'Agency_code','Source','ticket_type','Coupon_used')\n",
    "bos_df_csv = bos_df_csv\\\n",
    "    .withColumn('attributes', fc_DV_2_list[0])\\\n",
    "    .withColumn('Agency_code', fc_DV_2_list[1])\\\n",
    "    .withColumn('Source', fc_DV_2_list[2])\\\n",
    "    .withColumn('ticket_type', fc_DV_2_list[3])\\\n",
    "    .withColumn('Coupon_used', fc_DV_2_list[4])"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "sc = SparkContext.getOrCreate();\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "my_conn_options = {\n",
    "    \"dbtable\": \"flextravel.fx_trans_file\",\n",
    "    \"database\": \"fts_cp_uat\",\n",
    "    \"url\": \"jdbc:postgresql://flextravel-uat-serverless-pg.chzjoncadzav.ca-central-1.rds.amazonaws.com:5432/fts_cp_uat\",\n",
    "    \"customJdbcDriverS3Path\":\"s3://flex-data-uat-canda-central/DEV/postgresql-42.6.0.jar\",\n",
    "    \"customJdbcDriverClassName\":\"org.postgresql.Driver\",\n",
    "    \"user\":\"flexuatuser\",\n",
    "    \"password\":\"flexnewyearpwd@2022\"\n",
    "}\n",
    "\n",
    "my_conn_options1 = {\n",
    "    \"dbtable\": \"flextravel.general_info\",\n",
    "    \"database\": \"fts_cp_uat\",\n",
    "    \"url\": \"jdbc:postgresql://flextravel-uat-serverless-pg.chzjoncadzav.ca-central-1.rds.amazonaws.com:5432/fts_cp_uat\",\n",
    "    \"customJdbcDriverS3Path\":\"s3://flex-data-uat-canda-central/DEV/postgresql-42.6.0.jar\",\n",
    "    \"customJdbcDriverClassName\":\"org.postgresql.Driver\",\n",
    "    \"user\":\"flexuatuser\",\n",
    "    \"password\":\"flexnewyearpwd@2022\"\n",
    "}\n",
    "\n",
    "my_conn_options2 = {\n",
    "    \"dbtable\": \"public.fx_trans_bre_interim_bos_sprint2_test\",\n",
    "    \"database\": \"fts_cp_uat\",\n",
    "    \"url\": \"jdbc:postgresql://flextravel-uat-serverless-pg.chzjoncadzav.ca-central-1.rds.amazonaws.com:5432/fts_cp_uat\",\n",
    "    \"customJdbcDriverS3Path\":\"s3://flex-data-uat-canda-central/DEV/postgresql-42.6.0.jar\",\n",
    "    \"customJdbcDriverClassName\":\"org.postgresql.Driver\",\n",
    "    \"user\":\"flexuatuser\",\n",
    "    \"password\":\"flexnewyearpwd@2022\"\n",
    "}\n",
    "\n",
    "df = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"postgresql\",\n",
    "    connection_options=my_conn_options,\n",
    "    transformation_ctx=\"df\",\n",
    ")\n",
    "\n",
    "df_GI = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"postgresql\",\n",
    "    connection_options=my_conn_options1,\n",
    "    transformation_ctx=\"df\",\n",
    ")\n",
    "GI_df = df_GI.toDF().select(\"id\",\"tids_code\")  \n",
    "\n",
    "\n",
    "\n",
    "bos_df_csv=  bos_df_csv.withColumn( \"Orginal_currency\",F.when(length(col(\"Org_currency\"))>5,'').otherwise(bos_df_csv.Org_currency))\n",
    "\n",
    "\n",
    "bos_df_final = bos_df_csv.select(\"ticket_type\",\"org_ticket_no\",\"version_no\",\"Filename\",\"Transaction_Date\",\"Agency_code\",\"Source\",\"Booking_channel\",\"Ticket_No\",\"PNR\",\"Issue_date\",\"Tour_code\",\"Passenger_name\",\"Passenger_Count\",\"Coupon_used\",\"Country_code\",\"Legs\",\"Original_Fare\",\"Orginal_currency\",\"exch_rate\",\"Fare_amt\",\"Tax_Amt\",\"Total_amt\",\"curr_code\",\"FOP\",\"Tax\",\"Commission\",\"Fare_Cons\",\"attributes\",\"file_path\")\n",
    "                                                           \n",
    "Bos_df_GI= bos_df_final.join(GI_df,\n",
    "               bos_df_csv.Agency_code == GI_df.tids_code, \n",
    "               \"left\")  \n",
    "\n",
    "\n",
    "Bos_df_GI=Bos_df_GI.withColumnRenamed(\"id\",\"agent_id\")\n",
    "\n",
    "#Bos_df_GI.show(2, truncate=False)\n",
    "\n",
    "# df1 = df.toDF().select(\"id\", \"supplier_id\",\"sup_srv_map_id\",\"file_name\").where(df[\"file_name\"] =='06.07_DCALLRECORDSAM')\n",
    "# #df2=df1.withColumnRenamed(\"sup_srv_map_id\",\"col0\").withColumnRenamed(\"file_name\",\"col1\")\n",
    "# # df1 = df.filter(f=lambda x: x[\"sup_srv_map_id\"] in [65])\n",
    "\n",
    "df1 = df.toDF()[\"id\", \"supplier_id\",\"sup_srv_map_id\",\"file_name\"]\n",
    "df1 = df1[df1[\"file_name\"] =='06.07_DCALLRECORDSAM']\n",
    "\n",
    "\n",
    "#df1.show()\n",
    "\n",
    "Bos_df_file= Bos_df_GI.join(df1,\n",
    "               bos_df_csv.Filename == df1.file_name, \n",
    "               \"left\")    \n",
    "\n",
    "# Bos_df_file.show()\n",
    "\n",
    "# FTS - 2598 [\"Tax_Amt\"].cast(IntegerType()) -> DoubleType, [\"Total_amt\"].cast(IntegerType()) -> DoubleType\n",
    "Bos_df_file= Bos_df_file.withColumn(\"Original_Fare\", Bos_df_file[\"Original_Fare\"].cast(DoubleType())).withColumn(\"Fare_amt\", Bos_df_file[\"Fare_amt\"].cast(DoubleType())).withColumn(\"Tax_Amt\", Bos_df_file[\"Tax_Amt\"].cast(DoubleType()))\\\n",
    "            .withColumn(\"Total_amt\", Bos_df_file[\"Total_amt\"].cast(DoubleType())) .withColumn(\"exch_rate\", Bos_df_file[\"exch_rate\"].cast(DoubleType()))\n",
    "\n",
    "Bos_df_file= Bos_df_file.withColumn(\"Commission_Amount\", Bos_df_file[\"Fare_amt\"].cast(DoubleType()))\n",
    "                        \n",
    "Bos_df_write = Bos_df_file.select(\"Transaction_Date\",\"Agency_code\",\"Source\",\"Booking_channel\",\"Ticket_No\",\"PNR\",\"Issue_date\",\"Tour_code\",\"Passenger_name\",\"Passenger_Count\",\"Coupon_used\",\"Country_code\",\"Legs\",\"Original_Fare\",\"Orginal_currency\",\"exch_rate\",\"Fare_amt\",\"Tax_Amt\",\"Total_amt\",\"curr_code\",\"FOP\",\"Tax\",\"Commission\",\"Fare_Cons\",\"agent_id\",\"id\",\"supplier_id\",\"sup_srv_map_id\",\"ticket_type\",\"version_no\",\"org_ticket_no\",\"Commission_Amount\",\"attributes\",\"file_path\")\n",
    "\n",
    "\n",
    "Bos_df_write = Bos_df_write.withColumnRenamed(\"Transaction_Date\",\"transaction_date\").withColumnRenamed(\"Agency_code\",\"agency_code\").withColumnRenamed(\"Source\",\"source\").withColumnRenamed(\"id\",\"trans_file_id\")\\\n",
    "                .withColumnRenamed(\"Booking_channel\",\"booking_channel\").withColumnRenamed(\"Ticket_No\",\"ticket_number\").withColumnRenamed(\"PNR\",\"pnr\").withColumnRenamed(\"Issue_date\",\"issue_date\").withColumnRenamed(\"Tour_code\",\"tour_code\")\\\n",
    "                .withColumnRenamed(\"Passenger_name\",\"passenger_name\").withColumnRenamed(\"Passenger_Count\",\"passenger_count\").withColumnRenamed(\"Coupon_used\",\"coupon_used\").withColumnRenamed(\"Country_code\",\"country_code\")\\\n",
    "                .withColumnRenamed(\"Legs\",\"legs\").withColumnRenamed(\"Original_Fare\",\"original_fare\").withColumnRenamed(\"Orginal_currency\",\"original_currency\").withColumnRenamed(\"exch_rate\",\"exchange_rate\").withColumnRenamed(\"Fare_amt\",\"fare_amount\")\\\n",
    "                .withColumnRenamed(\"Commission_Amount\",\"commission_amount\").withColumnRenamed(\"Tax_Amt\",\"tax_amount\").withColumnRenamed(\"Total_amt\",\"total_amount\").withColumnRenamed(\"curr_code\",\"currency_code\").withColumnRenamed(\"FOP\",\"payment\")\\\n",
    "                .withColumnRenamed(\"Tax\",\"tax\") .withColumnRenamed(\"Commission\",\"commission\").withColumnRenamed(\"Fare_Cons\",\"fare_construction\").withColumnRenamed(\"file_path\",\"refund_legs\")\n"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Bos_df_write_dyn=DynamicFrame.fromDF(Bos_df_write,glueContext,'Bos_df_write_dyn')\n",
    "\n",
    "df3=glueContext.write_dynamic_frame_from_options(\n",
    "    frame=Bos_df_write_dyn,\n",
    "    connection_type=\"postgresql\",\n",
    "    connection_options=my_conn_options2,\n",
    "    transformation_ctx=\"dynamic_frame\"\n",
    ")\n",
    "#df3.show()"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "PythonException: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/util.py\", line 87, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 30, in fc_Fare\nUnboundLocalError: local variable 'fare' referenced before assignment\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# # mini batch test\n",
    "# from pyspark.sql import SQLContext\n",
    "# sqlContest = SQLContext(sc)\n",
    "\n",
    "# i = 0\n",
    "# k = 19000\n",
    "# j = 100\n",
    "\n",
    "# while (i+j < k):\n",
    "#     print(i)\n",
    "\n",
    "#     try:\n",
    "#         pdk = Bos_df_write.toPandas()\n",
    "#         pd1 = pdk.loc[i:i+j][:].fillna(0)\n",
    "#         spark_df = sqlContest.createDataFrame(pd1)\n",
    "        \n",
    "#         Bos_df_write_dyn1=DynamicFrame.fromDF(spark_df,glueContext,'Bos_df_write_dyn')\n",
    "\n",
    "#         df3=glueContext.write_dynamic_frame_from_options(\n",
    "#             frame=Bos_df_write_dyn1,\n",
    "#             connection_type=\"postgresql\",\n",
    "#             connection_options=my_conn_options2,\n",
    "#             transformation_ctx=\"dynamic_frame\"\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#     finally:\n",
    "#         i = i + j\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.3 \nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::251954244960:role/aws-glue-role\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 94e43814-4481-46a2-bd42-9a375b67a951\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.3\n--enable-glue-datacatalog true\nWaiting for session 94e43814-4481-46a2-bd42-9a375b67a951 to get into ready status...\nSession 94e43814-4481-46a2-bd42-9a375b67a951 has been created.\n\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}