{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "Python_Glue_Session",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "pygments_lexer": "python3",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import length,col, explode, upper, to_date, date_sub, lag, coalesce, lit, array_sort, when, arrays_zip, size, date_format, explode_outer, from_json, concat, expr, array\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from operator import itemgetter\n",
    "import datetime, re, requests\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from datetime import datetime\n",
    "# from boto3 import client\n",
    "# from boto3.dynamodb.conditions import Key"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# # read data from S3 to DataFrame\n",
    "# # input: S3\n",
    "# # output df['ROW_ID', 'BOS_FILE_EXTRACT', 'file_path', 'COLUMN_DEF']\n",
    "# def read_s3_to_df(day_path):\n",
    "#\n",
    "#     # parameter\n",
    "#     i = 0 # flag for initialize dataframe\n",
    "#\n",
    "#     # boto3 client\n",
    "#     s3 = boto3.client('s3')\n",
    "#     conn = client('s3')  # again assumes boto.cfg setup, assume AWS S3\n",
    "#\n",
    "#     # read data file by file\n",
    "#     for key in conn.list_objects(Bucket='bos-etl')['Contents']:\n",
    "#         path_key = key['Key']\n",
    "#         if path_key.endswith('cleansed'):\n",
    "#             if day_path in path_key:\n",
    "#                 file = s3.get_object(Bucket='bos-etl', Key=path_key)\n",
    "#                 txt = (file['Body'].read().decode('latin1'))\n",
    "#                 #st_re = txt.replace(\"\", \",\")\n",
    "#                 st_re_newline = txt.replace(\"!!! EOS !!!\", \"\\n\")\n",
    "#                 st_re_split = st_re_newline.split(\"\\n\")\n",
    "#                 df = pd.DataFrame(st_re_split)\n",
    "#                 df.index.name = 'ROW_ID'\n",
    "#                 df.rename({0:'BOS_FILE_EXTRACT'},axis='columns',inplace=True)\n",
    "#                 df[\"COLUMN_DEF\"]=df['BOS_FILE_EXTRACT'].replace(regex=r\"\\.*\",value=\"\")\n",
    "#                 rslt_dfRFT_temp = df[df['COLUMN_DEF'] =='PAX']\n",
    "#                 # rslt_dfRFT_temp = df\n",
    "#                 if ~rslt_dfRFT_temp.empty:\n",
    "#                     # print(path_key)\n",
    "#                     rslt_dfRFT_temp.insert(0,'file_path', path_key)\n",
    "#                     if (i ==0):\n",
    "#                         rslt_dfRFT = rslt_dfRFT_temp\n",
    "#                         i = 1\n",
    "#                     else:\n",
    "#                         rslt_dfRFT = pd.concat([rslt_dfRFT,rslt_dfRFT_temp])\n",
    "#     return rslt_dfRFT\n",
    "#\n",
    "#\n",
    "# # read data as DataFrame\n",
    "# #select day\n",
    "# day_path = 'date=2023-06-17'\n",
    "# # select current system day\n",
    "# # day_path = time.strftime('%Y-%m-%d')\n",
    "# rslt_dfRFT = read_s3_to_df(day_path)\n",
    "#\n",
    "# # test\n",
    "# #filter content\n",
    "# # check_list=['123','456']\n",
    "#\n",
    "# # for i in range():\n",
    "# #       rslt_dfRFT['BOS_FILE_EXTRACT'].filter(like=check_list[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# #test write to s3 csv\n",
    "# from io import StringIO\n",
    "# bucket = 'bos-etl' # already created on S3\n",
    "# csv_buffer = StringIO()\n",
    "# rslt_dfRFT.to_csv(csv_buffer)\n",
    "# s3_resource = boto3.resource('s3')\n",
    "# s3_resource.Object(bucket, 'write_back/df.csv').put(Body=csv_buffer.getvalue())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# read csv\n",
    "rslt_dfRFT = pd.read_csv(\"source/input/1_df_RFT_PAT_PAX.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from awsglue.transforms import *\n",
    "# from awsglue.utils import getResolvedOptions\n",
    "# from pyspark.context import SparkContext\n",
    "# from awsglue.context import GlueContext\n",
    "# from awsglue.job import Job\n",
    "#\n",
    "# ## @params: [JOB_NAME]\n",
    "# args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "#\n",
    "# sc = SparkContext()\n",
    "# glueContext = GlueContext(sc)\n",
    "# spark = glueContext.spark_session\n",
    "# job = Job(glueContext)\n",
    "# job.init(args['JOB_NAME'], args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/07 19:53:09 WARN Utils: Your hostname, Rongs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.10 instead (on interface en0)\n",
      "23/08/07 19:53:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/07 19:53:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Pyspark Dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "#Create PySpark SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "#Create PySpark DataFrame from Pandas\n",
    "sparkDF=spark.createDataFrame(rslt_dfRFT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# # Split Raw Data\n",
    "#\n",
    "# def split_raw_data(sparkDF):\n",
    "#     df1 = sparkDF.withColumn(\"split\", F.split(\"BOS_FILE_EXTRACT\", \"<\")).withColumn(\"ITI\", F.element_at(\"split\", 2)).withColumn(\"FAR\", F.element_at(\"split\", 3))\\\n",
    "#     .withColumn(\"FOP_AND_FAR2\", F.element_at(\"split\", 4)).withColumn(\"END\", F.element_at(\"split\", 5)).withColumn(\"CER\", F.element_at(\"split\", 6))\\\n",
    "#     .withColumn(\"EXC\", F.element_at(\"split\", 7))\\\n",
    "#     .withColumn(\"split2\", F.split(\"BOS_FILE_EXTRACT\", \"<\")).withColumn(\"DCI\", F.element_at(\"split2\", 1)).withColumn(\"SAL\", F.element_at(\"split2\", 2))\\\n",
    "#     .withColumn(\"EXS\", F.element_at(\"split2\", -1))\\\n",
    "#     .withColumn(\"split3\", F.split(\"BOS_FILE_EXTRACT\", \"<\")).withColumn(\"TAX2\", F.element_at(\"split3\", 2))\\\n",
    "#     .withColumn(\"split4\", F.split(\"BOS_FILE_EXTRACT\", \"<\")).withColumn(\"TAX1\", F.element_at(\"split4\", 2))\n",
    "#\n",
    "#     split_df = df1.select(\"COLUMN_DEF\",\"DCI\",\"SAL\",\"TAX1\",\"TAX2\",\"ITI\",\"FAR\",\"FOP_AND_FAR2\",\"END\",\"CER\",\"EXC\",\"EXS\",split(df1.TAX1, '<').alias('split_text'),split(df1.FOP_AND_FAR2,'N<').alias(\"Splittext3\"),\"file_path\",\"TAX2\",F.regexp_extract(df1.BOS_FILE_EXTRACT, '<.+',0).alias('RFT'))\n",
    "#     split_df_type=split_df.selectExpr(\"column_def\",\"DCI\",\"SAL\",\"concat(split_text[0],',',TAX2) as TAX\",\"ITI\",\"FAR\", \"Case when column_def=='PAT' then FOP_AND_FAR2 else Splittext3[1] end as FOP\",\"Case when column_def=='PAT' then CER else END end as END_T\",\"Case when column_def=='PAT' then EXC else CER end as CER_T\",\"Case when column_def=='PAT' then '' else EXC end as EXC\",\"Case when column_def=='PAT' then '' else CER end as EXS_T\",\"file_path\",\"TAX2\",\"RFT\")\n",
    "#\n",
    "#     return split_df_type\n",
    "#\n",
    "# split_df_type = split_raw_data(sparkDF)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# Split Raw Data\n",
    "#  Start of repeat\n",
    "#  END REPEAT\n",
    "#  END of Data Set Indicator\n",
    "# <   Carrriage Return\n",
    "# DCI < SAL < TAX < ITI < FAR < FOP < END < CER < EXC < EXS <\n",
    "#     < REF <\n",
    "\n",
    "def split_raw_data(sparkDF):\n",
    "    split_df_type = sparkDF.withColumn(\"split\", F.split(\"BOS_FILE_EXTRACT\", \"\"))\\\n",
    "                 .withColumn(\"DCI_SAL\", F.element_at(\"split\", 1))\\\n",
    "                 .withColumn(\"split2\", F.split(\"DCI_SAL\", \"<\"))\\\n",
    "                 .withColumn(\"DCI\", F.element_at(\"split2\", 1))\\\n",
    "                 .withColumn(\"SAL\", F.element_at(\"split2\", 2))\\\n",
    "                 .withColumn(\"TAX\", F.element_at(\"split\", 2))\\\n",
    "                 .withColumn(\"ITI\", F.element_at(\"split\", 3))\\\n",
    "                 .withColumn(\"FAR\", F.element_at(\"split\", 4))\\\n",
    "                 .withColumn(\"FOP\", F.element_at(\"split\", 5))\\\n",
    "                 .withColumn(\"END\", F.element_at(\"split\", 6))\\\n",
    "                 .withColumn(\"CER\", F.element_at(\"split\", 7))\\\n",
    "                 .withColumn(\"EXC\", F.element_at(\"split\", 8))\\\n",
    "                 .withColumn(\"EXS\",  F.element_at(\"split\", 9))\\\n",
    "                 .withColumn(\"RFT\",  F.regexp_extract('BOS_FILE_EXTRACT', '<.+<',0))\n",
    "\n",
    "    split_df_type = split_df_type.select('COLUMN_DEF', \"DCI\", \"SAL\", \"TAX\", \"ITI\", \"FAR\", \"FOP\",\"END\", \"CER\", \"EXC\", \"EXS\", \"RFT\")\n",
    "\n",
    "    return split_df_type\n",
    "\n",
    "split_df_type = split_raw_data(sparkDF)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(column_def='RFT', DCI='RFT\\x8022521623\\x8020230618\\x8023061805093326\\x80125\\x802161454822\\x804\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80', SAL='\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80', TAX='', ITI=None, FAR=None, FOP=None, END=None, CER=None, EXC=None, EXS=None, RFT='<\\x80\\x80\\x80\\x80\\x80953.65\\x800.00\\x800.00\\x80358.00\\x80595.65\\x800.00\\x800.00\\x800.00\\x800.00\\x80\\x80\\x80\\x81125\\x802161454822\\x801234\\x80<\\x82\\x80REDACTED REF NAME\\x80F\\x80-953.65\\x800.00\\x800.00\\x80\\x80-953.65\\x80CA\\x80CASH\\x80<\\x83'),\n Row(column_def='PAX', DCI='PAX\\x8022521623\\x8020230618\\x8023061805093326\\x80001\\x807976703339\\x800\\x8017JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80', SAL='\\x80\\x80\\x80\\x80\\x80\\x80FFVV\\x80203.20\\x80166.51\\x80USD\\x800.00\\x8036.69\\x801.00\\x800.60\\x80HNKNVK/AA\\x80REDACTED SAL NAME\\x80\\x80\\x80\\x800011\\x80/\\x80\\x80Y\\x80\\x80\\x800\\x80\\x80\\x8000\\x80', TAX='\\x8112.49\\x80US\\x80<9.60\\x80ZP\\x80<5.60\\x80AY\\x80<9.00\\x80XF\\x80<\\x82\\x81ORF\\x804.5\\x80<DCA\\x804.5\\x80<\\x82', ITI='\\x81\\x80\\x80X\\x80ORF\\x80DCA\\x80AA\\x805508\\x80S\\x8008JUL\\x80323P\\x80SUAIZNN1\\x80\\x80DCA\\x80GRR\\x80AA\\x805091\\x80S\\x8008JUL\\x80530P\\x80SUAIZNN1\\x80V\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80V\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80<\\x82', FAR='\\x81ORF AA X/WAS AA GRS166.51USD166.51END ZPORFDCA XFORF4.5DCA4.5\\x80<\\x82', FOP='\\x81CA\\x80CASH\\x80N \\x80\\x80\\x80203.20\\x80\\x80\\x800.00\\x80\\x801.00\\x800.60\\x80N\\x80<CA\\x80CASH\\x80N\\x80\\x80\\x8025.00\\x80\\x80\\x800.00\\x80\\x801.00\\x800.60\\x80E\\x80<\\x82', END='\\x81USD166.51 NONREFUNDABLE/NONREF/FAREDIF/CXL BY FLT TIME OR NOVALUE\\x80<\\x82', CER='\\x81\\x80<\\x82', EXC='\\x81001\\x807976703339\\x800\\x80<\\x82\\x81001\\x807965804275\\x802\\x8020230430\\x8022521623\\x80C0010E0G6XH48Y\\x80143.26\\x80178.20\\x801.00\\x800.70\\x8034.94\\x800.00\\x800.00\\x80\\x8112\\x80<\\x82F\\x80REDACTED EXC NAME\\x80<\\x82', EXS='143.26\\x80166.51\\x8034.94\\x8036.69\\x80178.20\\x80203.20\\x8025.00\\x801.00\\x801.00\\x800.00\\x800.00\\x800.00\\x801\\x800.00\\x8025.00\\x80<', RFT='<\\x80\\x80\\x80\\x80\\x80\\x80FFVV\\x80203.20\\x80166.51\\x80USD\\x800.00\\x8036.69\\x801.00\\x800.60\\x80HNKNVK/AA\\x80REDACTED SAL NAME\\x80\\x80\\x80\\x800011\\x80/\\x80\\x80Y\\x80\\x80\\x800\\x80\\x80\\x8000\\x80<\\x83\\x8112.49\\x80US\\x80<9.60\\x80ZP\\x80<5.60\\x80AY\\x80<9.00\\x80XF\\x80<\\x82\\x81ORF\\x804.5\\x80<DCA\\x804.5\\x80<\\x82\\x83\\x81\\x80\\x80X\\x80ORF\\x80DCA\\x80AA\\x805508\\x80S\\x8008JUL\\x80323P\\x80SUAIZNN1\\x80\\x80DCA\\x80GRR\\x80AA\\x805091\\x80S\\x8008JUL\\x80530P\\x80SUAIZNN1\\x80V\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80V\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80<\\x82\\x83\\x81ORF AA X/WAS AA GRS166.51USD166.51END ZPORFDCA XFORF4.5DCA4.5\\x80<\\x82\\x83\\x81CA\\x80CASH\\x80N \\x80\\x80\\x80203.20\\x80\\x80\\x800.00\\x80\\x801.00\\x800.60\\x80N\\x80<CA\\x80CASH\\x80N\\x80\\x80\\x8025.00\\x80\\x80\\x800.00\\x80\\x801.00\\x800.60\\x80E\\x80<\\x82\\x83\\x81USD166.51 NONREFUNDABLE/NONREF/FAREDIF/CXL BY FLT TIME OR NOVALUE\\x80<\\x82\\x83\\x81\\x80<\\x82\\x83\\x81001\\x807976703339\\x800\\x80<\\x82\\x81001\\x807965804275\\x802\\x8020230430\\x8022521623\\x80C0010E0G6XH48Y\\x80143.26\\x80178.20\\x801.00\\x800.70\\x8034.94\\x800.00\\x800.00\\x80\\x8112\\x80<\\x82F\\x80REDACTED EXC NAME\\x80<\\x82\\x83143.26\\x80166.51\\x8034.94\\x8036.69\\x80178.20\\x80203.20\\x8025.00\\x801.00\\x801.00\\x800.00\\x800.00\\x800.00\\x801\\x800.00\\x8025.00\\x80<\\x83'),\n Row(column_def='PAT', DCI='PAT\\x8022521623\\x8020230618\\x8023061805093326\\x80074\\x802100377105\\x803\\x8016JUN23\\x80\\x80IAR\\x80BA/D/ALLRECORDS/18JUN23\\x80103\\x80', SAL='\\x80\\x80\\x80\\x80\\x80\\x80FFVV\\x80302.20\\x80213.00\\x80USD\\x800.00\\x8089.20\\x800.00\\x800.00\\x80UOAGUL\\x80REDACTED SAL NAME\\x80ITAFKL\\x80\\x800005WS\\x800744\\x80/\\x80\\x807\\x80\\x80\\x800\\x80X\\x80\\x8000\\x80', TAX='\\x812.20\\x80YR\\x80<16.00\\x80CJ\\x80<22.50\\x80RN\\x80<28.60\\x80VV\\x80<15.70\\x80JD\\x80<0.70\\x80OG\\x80<3.50\\x80QV\\x80<\\x82\\x81\\x80\\x80<\\x82', ITI='\\x81\\x80\\x80O\\x80AMS\\x80MAD\\x80KL\\x801701\\x80Q\\x8024JUN\\x800930\\x80QH5UA5LG/XX\\x80O\\x80MAD\\x80AMS\\x80KL\\x801702\\x80E\\x8027JUN\\x801305\\x80EH5UA5LG\\x80V\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80V\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80<\\x82', FAR='\\x81AMS KL MAD119.12KL AMS97.52NUC216.64END ROE0.907418XT22.50RN28.60VV15.70JD0.70OG3.50QV\\x80<\\x82', FOP='\\x81CA\\x80CASH\\x80N\\x80\\x80\\x80302.20\\x80\\x80\\x800.00\\x80\\x800.00\\x800.00\\x80N\\x80<\\x82', END='\\x81NDC BR 1.08168132\\x80<\\x82', CER='\\x81\\x80<\\x82', EXC='', EXS=None, RFT='<\\x80\\x80\\x80\\x80\\x80\\x80FFVV\\x80302.20\\x80213.00\\x80USD\\x800.00\\x8089.20\\x800.00\\x800.00\\x80UOAGUL\\x80REDACTED SAL NAME\\x80ITAFKL\\x80\\x800005WS\\x800744\\x80/\\x80\\x807\\x80\\x80\\x800\\x80X\\x80\\x8000\\x80<\\x83')]"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df_type.select('column_def',\"DCI\",\"SAL\",\"TAX\",\"ITI\",\"FAR\",\"FOP\",\"END\",\"CER\",\"EXC\",\"EXS\",\"RFT\").head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# Common\n",
    "# etl main fc\n",
    "def etl_fc(df, fc, input, output):\n",
    "    # fc\n",
    "    fc = udf(fc, ArrayType(StringType()))\n",
    "\n",
    "    # input\n",
    "    fc_list = fc(*input)\n",
    "\n",
    "    # output\n",
    "    for i in range(len(output)):\n",
    "        df = df.withColumn(output[i], fc_list[i])\n",
    "\n",
    "    return df\n",
    "\n",
    "# data validation fc\n",
    "def dv_fc(rule_name, args_name, args, msg, paras=''):\n",
    "\n",
    "    try:\n",
    "        if args is not None:\n",
    "            #rules\n",
    "            if rule_name == 'check_len':\n",
    "                # ValueOutOfRange\n",
    "                if '.' in args:\n",
    "                    (args, msg) = (args, msg) if (len(str(args).split('.')[0]) <= (paras[0] - paras[1])) & (len(str(args).split('.')[1]) < paras[1]) else ('NULL', msg + ' # ' + args_name +'OutOfRange: ' + str(args) + ' out of (' + str(paras[0]) + ',' + str(paras[1])  + ') # ')\n",
    "                elif re.search(r'^[0-9]+$',args):\n",
    "                        (args, msg) = (args, msg) if (len(str(args).split('.')[0]) <= (paras[0] - paras[1])) else ('NULL', msg + ' # ' + args_name +'OutOfRange: ' + str(args) + ' out of (' + str(paras[0]) + ',' + str(paras[1])  + ') # ')\n",
    "                else:\n",
    "                    (args, msg) = (args, msg) if len(str(args).split('.')[0]) <= paras[0] else ('NULL', msg + ' # ' + args_name +'OutOfRange: ' + str(args) + ' out of ' + str(paras[0]) + ') # ')\n",
    "\n",
    "            if rule_name == 'check_match':\n",
    "                # Value1MismatchValue2\n",
    "                msg = msg if args[0] == args[1] else msg + ' # ' + args_name[0] +'MisMatch' + args_name[1] + ': (' + args_name[0] + ' , ' + args_name[1]  + ') : (' + str(args[0]) + ' , '  + str(args[1]) + ') # '\n",
    "\n",
    "            if rule_name == 'check_empty':\n",
    "                #ValueIsEmpty\n",
    "                msg = msg if args != '' else ' # ' + args_name + 'isEmpty'  + ' # '\n",
    "\n",
    "        else:\n",
    "            # ValueIsNull\n",
    "            msg = msg + ' # ' + args_name + 'IsNone' + ' # '\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # ValueExceptin\n",
    "        msg = msg + ' # ' + args_name + 'Exception: ' + str(e) + ' # '\n",
    "\n",
    "    return (args, msg)\n",
    "\n",
    "# coupons format\n",
    "def coupons_format(s):\n",
    "    coupons = 10000\n",
    "    coupons_dict = {\"1\": 1000, \"2\": 200, \"3\": 30, \"4\": 4}\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        if s[i] in coupons_dict:\n",
    "            coupons = coupons + coupons_dict[s[i]]\n",
    "\n",
    "    return str(coupons)[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Custom\n",
    "# PAT/PAX/REF : [transaction_date, source, booking_channel, version_no, currency_code, ticket_type]\n",
    "# sc = SparkContext.getOrCreate();\n",
    "# glueContext = GlueContext(spark)\n",
    "#\n",
    "# my_conn_options_loader_map = {\n",
    "#     \"dbtable\": \"flextravelengine.fx_bre_loader_to_pos_map\",\n",
    "#     \"database\": \"fts_cp_uat\",\n",
    "#     \"url\": \"jdbc:postgresql://flextravel-uat-serverless-pg.chzjoncadzav.ca-central-1.rds.amazonaws.com:5432/fts_cp_uat\",\n",
    "#     \"customJdbcDriverS3Path\":\"s3://flex-data-uat-canda-central/DEV/postgresql-42.6.0.jar\",\n",
    "#     \"customJdbcDriverClassName\":\"org.postgresql.Driver\",\n",
    "#     \"user\":\"flexuatuser\",\n",
    "#     \"password\":\"flexnewyearpwd@2022\"\n",
    "# }\n",
    "#\n",
    "# df_loader_map = glueContext.create_dynamic_frame.from_options(\n",
    "#     connection_type=\"postgresql\",\n",
    "#     connection_options=my_conn_options_loader_map,\n",
    "#     transformation_ctx=\"df\",\n",
    "# ).toDF()\n",
    "\n",
    "\n",
    "def fc_custom(data):\n",
    "\n",
    "    # var\n",
    "    transaction_date = current_timestamp() # <<File Generated Date>>\n",
    "    source = 'BOS' # BOS\n",
    "    booking_channel = 'WEB' # WEB\n",
    "    version_no = 1\n",
    "    currency_code = 'USD' # always “USD”\n",
    "    # pos = df_loader_map.filter(df_loader_map['loader_name'] =='BOSSourceFileLoader').select('id').collect()[0][0]\n",
    "    pos = 1\n",
    "    ''' ticket_type :\n",
    "         line starts with PAT → ticket type = TKTT\n",
    "         line starts with PAX→ ticket type = EXCH-TKTT\n",
    "         line starts with RFT → ticket type = RFND\n",
    "         '''\n",
    "\n",
    "    # insert var\n",
    "    data = data.withColumn('transaction_date', transaction_date)\\\n",
    "               .withColumn('source', F.lit(source)) \\\n",
    "               .withColumn('booking_channel', F.lit(booking_channel))\\\n",
    "               .withColumn('version_no', F.lit(version_no))\\\n",
    "               .withColumn('ticket_type', when(col('column_def') == 'PAT', 'TKTT').\\\n",
    "                                          when(col('column_def') == 'PAX', 'EXCH-TKTT').\\\n",
    "                                          when(col('column_def') == 'RFT', 'RFND').\\\n",
    "                                          otherwise(''))\\\n",
    "               .withColumn('currency_code', F.lit(currency_code))\\\n",
    "               .withColumn('country_code', F.lit('USA'))\\\n",
    "               .withColumn('passenger_count', F.lit(1))\\\n",
    "               .withColumn('exception', F.lit(''))\\\n",
    "               .withColumn('pos', F.lit(pos))\n",
    "\n",
    "    return data\n",
    "\n",
    "bos_df_csv = fc_custom(split_df_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# DCI\n",
    "# PAT/PAX/REF : ['agency_code', 'ticket_number', 'issue_date']\n",
    "input  = ['column_def', 'DCI', 'exception']\n",
    "output = ['agency_code', 'ticket_number', 'issue_date', 'exception']\n",
    "\n",
    "def fc_DCI(column_def, DCI, exception):\n",
    "    # init\n",
    "    agency_code, ticket_number, issue_date = None, None, None\n",
    "\n",
    "    # DCI\n",
    "    if DCI is not None:\n",
    "        if column_def in ('PAT', 'PAX', 'RFT'):\n",
    "            # split\n",
    "            DCI_split = DCI.split(\"\")\n",
    "\n",
    "            # var\n",
    "            # agency_code = DCI_split[1] if len(DCI_split) >1 else None # 2nd field from 1st group (VLNC-DCI)\n",
    "            # ticket_number = DCI_split[4] + DCI_split[5] if len(DCI_split) >5 else None # 5th field + 6th field from 1st group (BACN-DCI+ BDNR-DCI)\n",
    "            # issue_date = datetime.strptime(DCI_split[7],'%d%b%y').strftime(\"%Y-%m-%d\") if len(DCI_split) >7 else None  # 8th field from 1st group (DAIS-DCI). Field is reported as YYMMDD\n",
    "            (agency_code, exception) = dv_fc('check_empty', 'agent_code', DCI_split[1], exception) if len(DCI_split) >1 else (None, exception)# 2nd field from 1st group (VLNC-DCI)\n",
    "            (ticket_number, exception) = dv_fc('check_empty', 'ticket_number', DCI_split[4] + DCI_split[5], exception) if len(DCI_split) >5 else (None, exception) # 5th field + 6th field from 1st group (BACN-DCI+ BDNR-DCI)\n",
    "            (issue_date, exception) = dv_fc('check_empty', 'issue_date', datetime.strptime(DCI_split[7],'%d%b%y').strftime(\"%Y-%m-%d\"),exception) if len(DCI_split) >7 else (None, exception)  # 8th field from 1st group (DAIS-DCI). Field is reported as YYMMDD\n",
    "\n",
    "    return [agency_code, ticket_number, issue_date, exception]\n",
    "\n",
    "bos_df_csv = etl_fc(bos_df_csv, fc_DCI, input, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+---------+\n",
      "|agency_code|ticket_number|issue_date|exception|\n",
      "+-----------+-------------+----------+---------+\n",
      "|   22521623|1252161454822|2023-06-17|         |\n",
      "|   22521623|0017976703339|2023-06-17|         |\n",
      "|   22521623|0742100377105|2023-06-16|         |\n",
      "+-----------+-------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bos_df_csv.select('agency_code', 'ticket_number', 'issue_date', 'exception').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# SAL\n",
    "# PAT/PAX : ['pnr', 'tour_code', 'passenger_name', 'coupon_used', 'original_fare', 'fare_amount', 'exchange_rate', 'commission_amount', 'original_currency', 'tax_amount', 'total_amount', 'commission','RFT']\n",
    "\n",
    "input  = ['column_def', 'SAL', 'RFT', 'exception']\n",
    "output = ['pnr', 'tour_code', 'passenger_name', 'coupon_used', 'original_fare', 'fare_amount', 'exchange_rate', 'commission_amount', 'original_currency', 'tax_amount', 'total_amount', 'commission', 'exception']\n",
    "\n",
    "def fc_SAL(column_def, SAL, RFT, exception):\n",
    "    # init\n",
    "    pnr, tour_code, passenger_name, coupon_used, original_fare, fare_amount, exchange_rate, commission_amount, original_currency, tax_amount, total_amount, commission = None, None, None, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "    # SAL\n",
    "    if SAL is not None:\n",
    "        if column_def == 'PAT' or column_def == 'PAX':\n",
    "            # split\n",
    "            SAL_split = SAL.split(\"\")\n",
    "\n",
    "            # var\n",
    "            pnr, exception = dv_fc('check_empty', 'pnr', SAL_split[14], exception)  # 15th field from 2nd group (PNRR-SAL). Note: it will be empty for RFND.\n",
    "            tour_code = SAL_split[16] # 17th field from 2nd group (TOUR-SAL). Note: it will be empty for RFND.\n",
    "            passenger_name = SAL_split[15]  # 16th field from 2nd group (PXNM-SAL) for non-RFND.\n",
    "            coupon_used = SAL_split[6]  # 7th field from 2nd group (CPUI-SAL) for non-RFND\n",
    "            original_fare = SAL_split[8]  # 9th field from 2nd group\n",
    "            original_currency = SAL_split[9] # 10th field from 2nd group (CUOF-SAL)\n",
    "            tax_amount = SAL_split[11]  # 12th field from 2nd group (TTAX-SAL)\n",
    "            total_amount = SAL_split[7]  # 8th field from 2nd group (TDAM-SAL)\n",
    "            fare_amount = SAL_split[10] if SAL_split[10] != '0.00' else original_fare # 11th field from 2nd group (EQFR-SAL)… Note: if is 0.00, use the same as ORIGINAL_FARE\n",
    "            exchange_rate = round(float(fare_amount) / float(original_fare),3) if float(original_fare) != 0 else None # FARE_AMOUNT / ORIGINAL_FARE\n",
    "            commission_amount = fare_amount # same as FARE_AMOUNT\n",
    "\n",
    "            ''' COMMISSION:\n",
    "            #JSON. Example: [{\"type\":\"BASE\",\"amount\":4.26,\"currency\":\"CAD\",\"commissionRate\":3.0}]\n",
    "                       amount: 13th field from 2nd group (COAM-SAL)\n",
    "                       commissionRate: 14th field from 2nd group (CORT-SAL)'''\n",
    "            amount = SAL_split[12]\n",
    "            commissionRate = SAL_split[13]\n",
    "            type1 = 'BASE'\n",
    "            currency = 'USD'\n",
    "\n",
    "            commission = '[{' + f'\"type\":\"{type1}\",\"amount\":{amount},\"currency\":\"{currency}\",\"commissionRate\":{commissionRate}' + '}]'\n",
    "\n",
    "    # RFT\n",
    "    if RFT is not None:\n",
    "        if column_def == 'RFT':\n",
    "            # split\n",
    "            RFT_split = RFT.split('<')\n",
    "            RFT1_split = RFT_split[0].replace('<', '') # RCAM RCRT\n",
    "            RFT1_element = RFT1_split.split('')\n",
    "\n",
    "            '''commission:\n",
    "            JSON. Example: [{\"type\":\"BASE\",\"amount\": -4.26,\"currency\":\"USD\",\"commissionRate\":3.0}]\n",
    "            type: BASE\n",
    "            amount: 11th field from REF * (-1) because commission is being refunded\n",
    "            commissionrate: 12th field from REF\n",
    "            currency: USD'''\n",
    "\n",
    "            # var\n",
    "            type1 = 'BASE'\n",
    "            amount = str(float(RFT1_element[10]) * -1) if float(RFT1_element[10]) != 0.00 else '0.00'\n",
    "            commissionRate = RFT1_element[11]\n",
    "            currency = 'USD'\n",
    "\n",
    "            commission = '[{' + f'\"type\":\"{type1}\",\"amount\":{amount},\"currency\":\"{currency}\",\"commissionRate\":{commissionRate}' + '}]'\n",
    "\n",
    "    # check empty\n",
    "    if commission == '[]':\n",
    "        _, exception = dv_fc('check_empty', 'commission', \"\", exception)\n",
    "\n",
    "    return [pnr, tour_code, passenger_name, coupon_used, original_fare, fare_amount, exchange_rate, commission_amount, original_currency, tax_amount, total_amount, commission, exception]\n",
    "\n",
    "bos_df_csv = etl_fc(bos_df_csv, fc_SAL, input, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+\n",
      "|column_def|      pnr|exception|\n",
      "+----------+---------+---------+\n",
      "|       RFT|     null|         |\n",
      "|       PAX|HNKNVK/AA|         |\n",
      "|       PAT|   UOAGUL|         |\n",
      "+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bos_df_csv.select('column_def','pnr','exception').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# TAX\n",
    "# PAT/PAX : ['tax']\n",
    "\n",
    "input  = ['column_def', 'TAX', 'original_currency', 'exception']\n",
    "output = ['tax', 'exception']\n",
    "\n",
    "def fc_TAX(column_def, TAX, original_currency, exception):\n",
    "    # init\n",
    "    tax = None\n",
    "\n",
    "    # TAX\n",
    "    if TAX is not None:\n",
    "        if column_def == 'PAT' or column_def == 'PAX':\n",
    "            # split\n",
    "            TAX_split = TAX.split('<')[0].replace('','').split('<')\n",
    "\n",
    "            # var\n",
    "            ''' TAX:\n",
    "            Field will be <null> if is RFND\n",
    "            Note: this group may have a loop\n",
    "            JSON. Example: [{\"type\":\"CA\",\"amount\":7.12,\"currency\":\"CAD\"},{\"type\":\"YR\",\"amount\":16.00,\"currency\":\"CAD\"}]\n",
    "            type: 2nd field of the loop from 3rd group (TMFT-TAX)\n",
    "            amount: 1st field of the loop from 3rd group (TMFA-TAX)\n",
    "            currency: 10th field from 2nd group (CUOF-SAL)'''\n",
    "            tax = ''\n",
    "            for i in range(len(TAX_split)):\n",
    "                # split\n",
    "                element = TAX_split[i].split('')\n",
    "\n",
    "                # element\n",
    "                type1 = element[1]\n",
    "                amount = element[0]\n",
    "                currency = original_currency\n",
    "\n",
    "                # tax\n",
    "                s = ',' if i != 0 else '['\n",
    "                tax = tax + s + '{' + f'\"type\":\"{type1}\",\"amount\":{amount},\"currency\":\"{currency}\"' + '}'\n",
    "\n",
    "            tax = tax + ']'\n",
    "\n",
    "            # check empty\n",
    "            if tax == '[]':\n",
    "                _, exception = dv_fc('check_empty', 'tax', \"\", exception)\n",
    "\n",
    "    return [tax, exception]\n",
    "\n",
    "bos_df_csv = etl_fc(bos_df_csv, fc_TAX, input, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "# ITI\n",
    "# PAT/PAX : ['legs']\n",
    "\n",
    "input  = ['column_def', 'ITI', 'ticket_number', 'currency_code', 'original_currency', 'issue_date', 'FAR', 'exception']\n",
    "output = ['legs', 'exception']\n",
    "\n",
    "def fc_ITI(column_def, ITI, ticket_number, currency_code, original_currency, issue_date, FAR, exception):\n",
    "\n",
    "    # init\n",
    "    legs = None\n",
    "\n",
    "    if column_def == 'PAT' or column_def == 'PAX':\n",
    "        # FAR\n",
    "        if FAR is not None:\n",
    "            FAR_split = re.findall(r'[A-PT-ZR\\s]\\d+.\\d{2}', FAR.split(\"END\")[0].replace(\">\",\"\"))\n",
    "            # Q_split = re.findall(r'[\\d\\s][a-zA-Z]\\d+\\.\\d+', FAR.split(\"END\")[0])\n",
    "            # Q_fare = [float(x[2:]) for x in Q_split ] # filter start alpha\n",
    "            # Q_fare_toal = sum(Q_fare) if len(Q_fare) != 0 else 0\n",
    "            #FAR_split = re.findall(r'[a-zA-Z]{3}[\\d\\.]+', re.sub(r'Q\\d+\\.\\d{2}', \"\", FAR.split(\"END\")[0]).replace(\" \",\"\"))\n",
    "            Q_split = re.findall(r'\\d+\\.\\d+', FAR.split(\"END\")[0])[:-1]\n",
    "            Q_fare = [float(x) for x in Q_split ] # filter start alpha\n",
    "            Q_fare_toal = round(sum(Q_fare),2) if len(Q_fare) != 0 else 0\n",
    "            if len(FAR_split) > 0:\n",
    "                FAR_split = [x[1:] for x in FAR_split ] # filter start alpha\n",
    "            if (len(FAR_split) > 1):\n",
    "                # if two more than two ticket legs, direction from right -> left\n",
    "                FAR_split.reverse()\n",
    "                # toal_far\n",
    "                fare_total = FAR_split[0]\n",
    "                # leg far\n",
    "                fare_legs = FAR_split[1:]\n",
    "            elif (len(FAR_split) == 1):\n",
    "                fare_total = FAR_split[0]\n",
    "                fare_legs = 0\n",
    "            else:\n",
    "                fare_total = 0\n",
    "                fare_legs = 0\n",
    "\n",
    "        # ITI\n",
    "        if ITI is not None:\n",
    "            # split\n",
    "            ITI_split = ITI.replace('','').replace('<','').split('<')\n",
    "            if (len(ITI_split)>1):\n",
    "                # if two more than two ticket legs, direction from right -> left\n",
    "                ITI_split.reverse()\n",
    "\n",
    "            # var\n",
    "            '''LEGS:\n",
    "            NOTE: If the ticket type is RFND, LEGS field needs to be empty\n",
    "            Note: this info is provided in the ITI group. Each ticket may have 4 legs max. After the 5th leg, the ticket is considered as conjunction. In the file, if there is a loop, the ticket has a conjunction ticket.Example: 1st leg, 2nd leg, 3rd leg, 4th leg + loop + 5th leg…If the leg is empty, it means that the ticket stopped in the previous leg. Don’t load empty values in the Json.\n",
    "            JSON. example: [{\"departure\":\"FLR\",\"destination\":\"YYZ\",\"seatClass\":\"C\",\"conjunction\":\"1111234567890\",\"carrier\":\"AC\",\"tripCode\":\"876\",\"departureOn\":\"2022-12-30\",\"designator\":\"\",\"stopOver\":\"X\",\"flyerCode\":\"\",\"fare\":259.25,\"currency\": \"CAD\",\"originalFare\":259.25,\"originalCurrency\":\"CAD\"}]\n",
    "            departure: from 4th group (ORAC-ITI) → Leg 1: 4th field | Leg 2: 13th field | Leg 3: 22nd field | Leg 4: 31st field\n",
    "            destination: from 4th group (DSTC-ITI) → Leg 1: 5th field | Leg 2: 14th field | Leg 3: 23rd field | Leg 4: 32nd field\n",
    "            seatClass: 4th group (CLSC-ITI) → Leg 1: 8th field | Leg 2: 17th field | Leg 3: 26th field | Leg 4: 35th field\n",
    "            conjunction: 1st field from 4th group (CJNR-ITI). If is empty, use the same as TICKET_NUMBER\n",
    "\n",
    "            if the ticket includes more than 4 legs: in ITI section, the 39th field (the field after 4th repeat’s designator) will give a new BDNR (10 digits), use the BACN from DCI + new BDNR as conjunction for the following legs\n",
    "            carrier: from 4th group (CARR-ITI) → Leg 1: 6th field | Leg 2: 15th field | Leg 3: 24th field | Leg 4: 33rd field\n",
    "            tripCode: 4th group (FTNR-ITI) → Leg 1: 7th field | Leg 2: 16th field | Leg 3: 25th field | Leg 4: 34th field\n",
    "            departured on: 4th group (FTDA-ITI) → Leg 1: 9th field | Leg 2: 18th field | Leg 3: 27th field | Leg 4: 36th field. NOTE: you need to store this format in the databse: YYYY-MM-DD but the file has JAN01 for example. Use the same procedure/logic from CAT file loader in order to convert into date\n",
    "\n",
    "            Check the logic from CAT (Java code) with Haibinhg and Santhosh because there are some tricks in the code but the logic is:\n",
    "            File will come as JUL01 (they don’t report the year) → convert into 2023-07-01\n",
    "            If the departure date is before the issue date, the year will be ISSUE_DATE +1 year. Example: issue date is 2023-07-01 and departure date is JUN01, then the departure date will be 2024-06-01\n",
    "            If the departure date is after or equals to the issue date, the year will be the same as Issue Date. Example: issue date is 2023-07-01 and departure date is DEC01, then the departure date will be 2023-12-01\n",
    "            If the departure date is after the issue date (but after december 31st), the year will be the same as Issue Date + 1 year. Example: issue date is 2023-07-01 and departure date is JAN01, then the departure date will be 2024-01-01\n",
    "            designator: 4th group (FBTD-ITI) → Leg 1: 11th field | Leg 2: 20th field | Leg 3: 29th field | Leg 4: 38th field\n",
    "            stopOver: from 4th group (STPO-ITI) → Leg 1: 3rd field | Leg 2: 12th field | Leg 3: 21st field | Leg 4: 30th field\n",
    "            flyerCode: <empty>\n",
    "            fare: you need to check the fare construction group and then apply the same logic as CAT loader (use the same rules applied on these stories: FTS-1518, FTS-1188 item 2, FTS-1188 and FTS-1502)\n",
    "            currency: same as CURRENCY_CODE\n",
    "            originalFare: you need to check the fare construction group and then apply the same logic as CAT loader (use the same rules applied on these stories: FTS-1518 and FTS-1188 item 2)\n",
    "            originalCurrency: same as ORIGINAL_CURRENCY'''\n",
    "\n",
    "            # if re.match(r'^\\d{4}-\\d{2}-\\d{2}$',str(issue_date)):\n",
    "            legs = ''\n",
    "            k = 0 # index for fare_legs segment\n",
    "            for i in range(len(ITI_split)):\n",
    "                # split\n",
    "                element = ITI_split[i].split('')\n",
    "\n",
    "                # segment\n",
    "                j = 30 # index for element\n",
    "                if (ticket_number is not None):\n",
    "                    conjunction = ticket_number if len(element[0]) == 0 else (ticket_number[0:3] + element[0])\n",
    "                    currency = currency_code\n",
    "                    originalCurrency = original_currency\n",
    "                    while (j > 0):\n",
    "                        if len(element[j]) != 0:\n",
    "                            # element\n",
    "                            departure = element[j]\n",
    "                            destination = element[j+1]\n",
    "                            seatClass = element[j+4]\n",
    "                            carrier = element[j+2]\n",
    "                            tripCode = element[j+3]\n",
    "                            designator = element[j+7]\n",
    "                            stopOver = element[j-1]\n",
    "                            flyerCode = ''\n",
    "\n",
    "                            # fare\n",
    "                            # far if 'X' != 0, take one from fare_leg\n",
    "                            if (stopOver == 'X'):\n",
    "                                fare = str('0.00')\n",
    "                            else:\n",
    "                                if (fare_legs == 0):\n",
    "                                    fare = 'NULL'\n",
    "                                elif (k < len(fare_legs)):\n",
    "                                    fare = fare_legs[k]\n",
    "                                    k = k + 1\n",
    "                                else:\n",
    "                                    fare = 'NULL'\n",
    "\n",
    "                            originalFare = fare\n",
    "\n",
    "                            # departureOn\n",
    "                            # dep_on_date(issue_year-mm-dd)if dep_on_date(mm-dd) > issue_data(mm-dd) else   dep_on_date((issue_year+1)-mm-dd))\n",
    "                            dep_on_date = element[j+5]\n",
    "                            if len(dep_on_date) != 0:\n",
    "                                if re.match(r'^\\d{4}-\\d{2}-\\d{2}$',str(issue_date)):\n",
    "                                    issue_year = issue_date[0:4]\n",
    "                                    dep_day = datetime.strptime(dep_on_date + '2024','%d%b%Y').strftime(\"%m-%d\") # 2024 is leap year to avoid   '02-28', just for transformation, not use it afterward\n",
    "                                    dep_year = str(issue_year) if dep_day >= issue_date[5:] else str(int(issue_year)+ 1)\n",
    "                                    departureOn = str(pd.datetime.strptime(dep_on_date + dep_year,'%d%b%Y'))[0:10]\n",
    "                                else:\n",
    "                                    departureOn = ''\n",
    "                            else:\n",
    "                                departureOn = ''\n",
    "\n",
    "                            # legs\n",
    "                            s = '[' if ((i == len(ITI_split) - 1) and (j == 3)) else ','\n",
    "                            legs = s + '{' + f'\"departure\":\"{departure}\",\"destination\":\"{destination}\",\"seatClass\":\"{seatClass}\",\"conjunction\":\"{conjunction}\",\"carrier\":\"{carrier}\",\"tripCode\":\"{tripCode}\",\"departureOn\":\"{departureOn}\",\"designator\":\"{designator}\",\"stopOver\":\"{stopOver}\",\"flyerCode\":\"{flyerCode}\",\"fare\":{fare},\"currency\":\"{currency}\",\"originalFare\":{originalFare},\"originalCurrency\":\"{originalCurrency}\"' + '}' + legs\n",
    "\n",
    "                        j = j - 9\n",
    "\n",
    "            legs = legs + ']'\n",
    "\n",
    "            # data validation\n",
    "            # check fare total\n",
    "            leg_fare = '%.2f'%sum([float(x[7:]) for x in re.findall(r'\\\"fare\\\"\\:\\d+\\.\\d+', legs)])\n",
    "            (args, exception) = dv_fc(rule_name = 'check_match', args_name = ('leg_fare','total_fare'), args = (Q_fare_toal,float(fare_total)), msg = exception, paras = None)\n",
    "\n",
    "            #check legs match fare\n",
    "            h = len(fare_legs) if fare_legs != 0 else 0\n",
    "            (args, exception) = dv_fc(rule_name = 'check_match', args_name = ('leg','fare'), args = (k, h), msg = exception, paras = None)\n",
    "\n",
    "            # check empty\n",
    "            if legs == '[]':\n",
    "                _, exception = dv_fc('check_empty', 'legs', \"\", exception)\n",
    "\n",
    "\n",
    "    return [legs, exception]\n",
    "\n",
    "bos_df_csv = etl_fc(bos_df_csv, fc_ITI, input, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# FAR\n",
    "# PAT/PAX : [fare_construction]\n",
    "\n",
    "input  = ['column_def', 'FAR', 'exception']\n",
    "output = ['fare_construction', 'exception']\n",
    "\n",
    "def fc_FAR(column_def, FAR, exception):\n",
    "    # init\n",
    "    fare_construction = None\n",
    "    if FAR is not None:\n",
    "        if column_def == 'PAT' or column_def == 'PAX':\n",
    "            # split\n",
    "            FAR_split =FAR.replace('','').replace('<','').split('<') # filter the last one which is ''\n",
    "\n",
    "            # var\n",
    "            '''FARE_CONSTRUCTION:\n",
    "            Note: the FAR group might have a loop, that’s why we need to use sequence 1, sequence 2, etc.\n",
    "            JSON. Example: [{\"sequence\":1,\"content\":\"AX373911153791006*0626/ 122948\"},{\"sequence\":2,\"content\":\"YHZ PD YMQ54.56CAD54.56END\"}]\n",
    "            content: 1st field from 5th group (FRCA-FAR)'''\n",
    "            fare_construction = ''\n",
    "            for i in range(len(FAR_split)):\n",
    "                element = FAR_split[i].split('')\n",
    "                content = element[0]\n",
    "                s = ',' if i != 0 else '['\n",
    "                fare_construction = fare_construction + s + '{' + f'\"sequence\":{i+1},\"content\":\"{content}\"' +'}'\n",
    "\n",
    "            fare_construction = fare_construction + ']'\n",
    "\n",
    "            # check empty\n",
    "            if fare_construction == '[]':\n",
    "                _, exception = dv_fc('check_empty', 'fare_construction', \"\", exception)\n",
    "\n",
    "    return [fare_construction, exception]\n",
    "\n",
    "bos_df_csv = etl_fc(bos_df_csv, fc_FAR, input, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# FOP\n",
    "# PAT/PAX : [payment]\n",
    "\n",
    "input  = ['column_def', 'FOP', 'currency_code', 'EXC', 'RFT', 'exception']\n",
    "output = ['payment']\n",
    "\n",
    "def fc_FOP(column_def, FOP, currency_code, EXC, RFT, exception):\n",
    "\n",
    "    # init\n",
    "    payment = None\n",
    "\n",
    "    # PAT PAX\n",
    "    if column_def == 'PAT' or column_def == 'PAX':\n",
    "        # EXC\n",
    "        if EXC is not None:\n",
    "            if column_def == 'PAX':\n",
    "                # split\n",
    "                EXC_split = EXC.split('<')\n",
    "                EXC2_split = EXC_split[1].replace('','').split('<')# loop of RACN + RDNR + CDGT\n",
    "                EXC2_element = EXC2_split[0].split(\"\")\n",
    "\n",
    "        # FOP\n",
    "        if FOP is not None:\n",
    "            # split\n",
    "            FOP_split = FOP.replace('','').replace('<','').split('<')\n",
    "\n",
    "            # var\n",
    "            '''PAYMENT:\n",
    "            JSON. Example: [{\"mode\":\"CC\",\"type\":\"CCXX\",\"amount\":0.00,\"accountNumber\":\"\",\"approvalCode\":\"\",\"invoiceNumber\":\"\",\"currency\":\"CAD\"}]\n",
    "            mode: 1st field of the loop from 7th group (FPTP-FOP). Use only the first 2 chars\n",
    "            type: 1st field of the loop from 7th group (FPTP-FOP)\n",
    "            amount: 6th field of the loop from 7th group (FPAM-FOP)\n",
    "            accountNumber: 2nd field of the loop from 7th group (FPAC-FOP)\n",
    "            approvalCode: 5th field of the loop from 7th group (APLC-FOP)\n",
    "            invoiceNumber: <empty>\n",
    "            currency: same as CURRENCY_CODE\n",
    "\n",
    "            for EXCH, the PAYMENT will be like the example below:\n",
    "            Besides the regular FOP above, we need to add the EX info. Example: [{\"mode\":\"EX\",\"type\":\"EX\",\"amount\":0.00,\"accountNumber\":\"451123456789001\",\"approvalCode\":\"\",\"invoiceNumber\":\"\",\"currency\":\"CAD\"},{\"mode\":\"CC\",\"type\":\"CCXX\",\"amount\":250.00,\"accountNumber\":\"\",\"approvalCode\":\"\",\"invoiceNumber\":\"\",\"currency\":\"CAD\"}]\n",
    "            accountNumber: 1st field + 2nd field + 3rd field + 19th field from 10th group (NACN-EXC+ NDNR-EXC + NCDT-EXC + RCPU-EXC). Note: RCPU is char and needs to be converted as number (use the same logic as “coupons field” from refund_legs.\n",
    "            amount: 0.00'''\n",
    "\n",
    "            '''PAX payment (json)\n",
    "            IF FPTP (Form of Payment Type) field is NOT empty\n",
    "            regular json object {} follows the same rules indicated in PAT\n",
    "            JSON. Example: [{\"mode\":\"CC\",\"type\":\"CCXX\",\"amount\":0.00,\"accountNumber\":\"\",\"approvalCode\":\"\",\"invoiceNumber\":\"\",\"currency\":\"CAD\"}]\n",
    "\n",
    "            IF FPTP (Form of Payment Type) field is empty\n",
    "            a json object as below\n",
    "            Example: {\"mode\":\"EX\",\"type\":\"EX\",\"amount\":0.00,\"accountNumber\":\"451123456789001\",\"approvalCode\":\"\",\"invoiceNumber\":\"\",\"currency\":\"USD\"}\n",
    "            mode:  EX\n",
    "            type: EX\n",
    "            amount: take the FPAM (Form of Payment AmounT) field from FOP section\n",
    "            accountNumber: EXC GROUP : 8th field (3 digit) + 9th field (10 digit) + 10th field (1 digit) + 22nd field  requirements confirmed\n",
    "            approvalCode: leave empty\n",
    "            invoiceNumber: leave empty\n",
    "            currency: same as CURRENCY_CODE\n",
    "            ** this condition is to be investigated and added to requirementafter 1st test\n",
    "            in case the original ticket has more than 4 legs, two { EX json } is required, because originating ticket has 2 conjunction number\n",
    "            1st {EX} :  the account number contains the first 4 legs' conjunction\n",
    "            2nd {EX} ; the account number contains the following legs' conjunction'''\n",
    "\n",
    "            payment = ''\n",
    "            currency = currency_code\n",
    "            for i in range(len(FOP_split)):\n",
    "                # split\n",
    "                FOP_element = FOP_split[i].split(\"\")\n",
    "\n",
    "                # payment\n",
    "                # element\n",
    "                mode , type1, accountNumber = '', '', ''\n",
    "                if len(FOP_element[0]) != 0:\n",
    "                    mode = FOP_element[0][:2]\n",
    "                    type1 = FOP_element[0]\n",
    "                    accountNumber = FOP_element[1]\n",
    "                else:\n",
    "                    mode = 'EX'\n",
    "                    type1 = 'EX'\n",
    "                    accountNumber = EXC2_element[0] + EXC2_element[1] + EXC2_element[2] + EXC2_element[13] if EXC2_element is not None else ''\n",
    "\n",
    "                amount = FOP_element[5] if len(FOP_element) >= 5 else ''\n",
    "                approvalCode = FOP_element[4] if len(FOP_element) >= 4 else ''\n",
    "                invoiceNumber = ''\n",
    "\n",
    "                s = ',' if i != 0 else '['\n",
    "                payment = payment + s + '{' + f'\"mode\":\"{mode}\",\"type\":\"{type1}\",\"amount\":{amount},\"accountNumber\":\"{accountNumber}\",\"approvalCode\":\"{approvalCode}\",\"invoiceNumber\":\"{invoiceNumber}\",\"currency\":\"{currency}\"' +'}'\n",
    "\n",
    "            payment = payment + ']'\n",
    "\n",
    "    # RFT\n",
    "    if RFT is not None:\n",
    "        if column_def == 'RFT':\n",
    "            RFT_split = RFT.split('<')\n",
    "            RFT2_split = RFT_split[1] # FPTP, FPAC, AMDU\n",
    "            RFT2_element = RFT2_split.split('')\n",
    "\n",
    "            '''REF payment:\n",
    "            JSON. Example: [{\"mode\":\"CC\",\"type\":\"CCVI4000\",\"amount\":0.00,\"accountNumber\":\"VI************5960\",\"approvalCode\":\"\",\"invoiceNumber\":\"\",\"currency\":\"USD\"}]\n",
    "            mode: 30th field from REF: use the first 2 characters only\n",
    "            type: 30th field from REF\n",
    "            amount: 26th field from REF\n",
    "            accountnumber: 31th field from REF\n",
    "            approvalcode: leave empty\n",
    "            invoicenumber: leave empty\n",
    "            currency: USD'''\n",
    "            mode = RFT2_element[8][:2]\n",
    "            type1 = RFT2_element[8]\n",
    "            amount = RFT2_element[3]\n",
    "            accountNumber = RFT2_element[9]\n",
    "            approvalCode = ''\n",
    "            invoiceNumber = ''\n",
    "            currency = 'USD'\n",
    "\n",
    "            payment = '[{' + f'\"mode\":\"{mode}\",\"type\":\"{type1}\",\"amount\":{amount},\"accountNumber\":\"{accountNumber}\",\"approvalCode\":\"{approvalCode}\",\"invoiceNumber\":\"{invoiceNumber}\",\"currency\":\"{currency}\"' +'}]'\n",
    "\n",
    "    # check empty\n",
    "    if payment == '[]':\n",
    "        _, exception = dv_fc('check_empty', 'payment', \"\", exception)\n",
    "\n",
    "    return [payment, exception]\n",
    "\n",
    "bos_df_csv = etl_fc(bos_df_csv, fc_FOP, input, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# RFT\n",
    "# RFT : [refund_legs]\n",
    "\n",
    "input  = ['column_def', 'RFT', 'ticket_number', 'EXC','coupon_used','exception']\n",
    "output = ['refund_legs', 'org_ticket_no','coupon_used','exception']\n",
    "\n",
    "def fc_RFT(column_def, RFT, ticket_number, EXC, coupon_used, exception):\n",
    "\n",
    "    # init\n",
    "    refund_legs, org_ticket_no = None, None\n",
    "\n",
    "    # RFT\n",
    "    if RFT is not None:\n",
    "        if column_def == 'RFT':\n",
    "            # split\n",
    "            RFT_split = RFT.split('<')\n",
    "            RFT_split_1 = re.search('.+<',RFT).group().replace('','').replace('<','').split('<') # loop RACN + RDNR + RCPN\n",
    "            RFT_split_2 = RFT_split[1] # ODOI\n",
    "\n",
    "            # var\n",
    "            # refund_legs\n",
    "            '''REFUND_LEGS:\n",
    "            note: to be populated if ticket_type is RFND only\n",
    "            JSON. example: [{\"sequence\":1,\"ticketNumber\":\"0011259634355\",\"coupons\":\"1000\",\"issueDate\":\"2022-05-03\"}]\n",
    "            ticketNumber: 17th field + 18th fied from 2nd group (RACN-REF+ RDNR-REF). Note: it may have a loop here, so you need to use the sequence 1, sequence 2, etc in the JSON\n",
    "            issueDate: 20th fied from 2nd group (ODOI-REF).\n",
    "            coupons: 19th fied from 2nd group (RCPN-REF). Note: we receive this field as chars. You need to convert into numbers, use the same logic as CAT Loader (for example, the field comes as RR, you need to convert into “1200”. Or might come as VRRV, for example, you need to convert into “0230”. You need to apply the number/sequence only for the letter R; to the other letters (or blank) you need to put “0”)'''\n",
    "\n",
    "            # element\n",
    "            issueDate = str(datetime.strptime(RFT_split_2.split(\"\")[0], \"%Y%m%d\").strftime(\"%Y-%m-%d\")) if len(RFT_split_2.split(\"\")[0]) != 0 else \"\"\n",
    "            for i in range(len(RFT_split_1)):\n",
    "                element = RFT_split_1[i].split(\"\")\n",
    "\n",
    "                # element\n",
    "                ticketNumber = element[0] + element[1]\n",
    "                coupons = coupons_format(element[2])\n",
    "                first_refund_legs_ticketNumber = ticketNumber if i == 0 else ''\n",
    "                first_refund_legs_coupon_used = coupons if i == 0 else ''\n",
    "\n",
    "                # refund_legs\n",
    "                refund_legs = ''\n",
    "                s = ',' if i != 0 else '['\n",
    "                refund_legs = refund_legs + s + '{' + f'\"sequence\":{str(i+1)},\"ticketNumber\":\"{ticketNumber}\",\"coupons\":\"{coupons}\",\"issueDate\":\"{issueDate}\"' +'}'\n",
    "\n",
    "            refund_legs = refund_legs + ']'\n",
    "\n",
    "            # coupon_used\n",
    "            coupon_used = first_refund_legs_coupon_used # For RFND → same as 1st “coupons field” from REFUND_LEGS\n",
    "\n",
    "            # org_ticket_no\n",
    "            org_ticket_no, exception = dv_fc('check_empty', 'org_ticket_no', first_refund_legs_ticketNumber, exception)\n",
    "\n",
    "            # check empty\n",
    "            if refund_legs == '[]':\n",
    "                _, exception = dv_fc('check_empty', 'refund_legs', \"\", exception)\n",
    "\n",
    "\n",
    "    # EXC\n",
    "    if EXC is not None:\n",
    "        if column_def == 'PAX':\n",
    "            # split\n",
    "            EXC_split = EXC.split('<')\n",
    "            EXC2_split = EXC_split[1].replace('','').split('<')# loop of RACN + RDNR\n",
    "            EXC2_element = EXC2_split[0].split(\"\")\n",
    "\n",
    "        # var\n",
    "        # org_ticket_no\n",
    "        '''ORG_TICKET_NO:\n",
    "            TKTT → same as TICKET_NUMBER\n",
    "            EXCH → 8th field from EXC (3 digit) + 9th field from EXC (10 digit) – this is the field that indicates the originating ticket subjected to exchange\n",
    "            RFND → same as 1st ticketNumber from REFUND_LEGS'''\n",
    "\n",
    "        if column_def == 'PAT':\n",
    "            org_ticket_no, exception = dv_fc('check_empty', 'org_ticket_no', ticket_number, exception)\n",
    "        elif column_def == 'PAX':\n",
    "            org_ticket_no, exception = dv_fc('check_empty', 'org_ticket_no', EXC2_element[0] + EXC2_element[1], exception)\n",
    "        else:\n",
    "            org_ticket_no = 'NULL'\n",
    "\n",
    "    return [refund_legs, org_ticket_no, coupon_used, exception]\n",
    "\n",
    "bos_df_csv = etl_fc(bos_df_csv, fc_RFT, input, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+---------+\n",
      "|coupon_used|column_def|org_ticket_no|exception|\n",
      "+-----------+----------+-------------+---------+\n",
      "|       1234|       RFT|1252161454822|         |\n",
      "|       FFVV|       PAX|0017965804275|         |\n",
      "|       FFVV|       PAT|0742100377105|         |\n",
      "+-----------+----------+-------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/871p0v1d2cn4c6y7bj_xghhh0000gn/T/ipykernel_1157/2218656264.py:123: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bos_df_csv.select('coupon_used','column_def','org_ticket_no', 'exception').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "'2023-06-17'"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"20230617\"\n",
    "str(datetime.strptime(s, \"%Y%m%d\").strftime(\"%Y-%m-%d\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains: 7",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m202306017\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mstr\u001B[39m(\u001B[43mdatetime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrptime\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mY\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43mm\u001B[39;49m\u001B[38;5;132;43;01m%d\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/envs/pyspark/lib/python3.8/_strptime.py:568\u001B[0m, in \u001B[0;36m_strptime_datetime\u001B[0;34m(cls, data_string, format)\u001B[0m\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_strptime_datetime\u001B[39m(\u001B[38;5;28mcls\u001B[39m, data_string, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%a\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mb \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    566\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001B[39;00m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;124;03m    format string.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 568\u001B[0m     tt, fraction, gmtoff_fraction \u001B[38;5;241m=\u001B[39m \u001B[43m_strptime\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_string\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    569\u001B[0m     tzname, gmtoff \u001B[38;5;241m=\u001B[39m tt[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m:]\n\u001B[1;32m    570\u001B[0m     args \u001B[38;5;241m=\u001B[39m tt[:\u001B[38;5;241m6\u001B[39m] \u001B[38;5;241m+\u001B[39m (fraction,)\n",
      "File \u001B[0;32m~/anaconda3/envs/pyspark/lib/python3.8/_strptime.py:352\u001B[0m, in \u001B[0;36m_strptime\u001B[0;34m(data_string, format)\u001B[0m\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime data \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m does not match format \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m    350\u001B[0m                      (data_string, \u001B[38;5;28mformat\u001B[39m))\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data_string) \u001B[38;5;241m!=\u001B[39m found\u001B[38;5;241m.\u001B[39mend():\n\u001B[0;32m--> 352\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munconverted data remains: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m    353\u001B[0m                       data_string[found\u001B[38;5;241m.\u001B[39mend():])\n\u001B[1;32m    355\u001B[0m iso_year \u001B[38;5;241m=\u001B[39m year \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    356\u001B[0m month \u001B[38;5;241m=\u001B[39m day \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mValueError\u001B[0m: unconverted data remains: 7"
     ]
    }
   ],
   "source": [
    "s = \"202306017\"\n",
    "str(datetime.strptime(s, \"%Y%m%d\").strftime(\"%Y-%m-%d\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "'2023-06-17'"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"20230617\"\n",
    "str(datetime.strptime(s, \"%Y%m%d\").strftime(\"%Y-%m-%d\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# data validation before insert DB\n",
    "# PAT/PAX/RFT : ['exception', 'original_fare', 'exchange_rate', 'fare_amount', 'tax_amount', 'total_amount', 'agency_code']\n",
    "\n",
    "input  = ['column_def', 'exception', 'original_fare', 'exchange_rate', 'fare_amount', 'tax_amount', 'total_amount', 'agency_code', 'source', 'coupon_used']\n",
    "output = ['exception', 'original_fare', 'exchange_rate', 'fare_amount', 'tax_amount', 'total_amount','agency_code', 'source', 'coupon_used']\n",
    "\n",
    "def fc(column_def, exception, original_fare,exchange_rate,fare_amount,tax_amount,total_amount, agency_code, source, coupon_used):\n",
    "\n",
    "    # len validatin\n",
    "    if column_def != 'RFT':\n",
    "        # original_fare\n",
    "        (original_fare, exception) = dv_fc(rule_name = 'check_len', args_name = 'original_fare', args = original_fare, msg = exception, paras=(12, 5))\n",
    "\n",
    "        # exchange_rate\n",
    "        (exchange_rate, exception) = dv_fc(rule_name = 'check_len', args_name = 'exchange_rate', args = exchange_rate, msg = exception, paras=(12, 5))\n",
    "\n",
    "        # fare_amount\n",
    "        (fare_amount, exception) = dv_fc(rule_name = 'check_len', args_name = 'fare_amount', args = fare_amount, msg = exception, paras=(12, 5))\n",
    "\n",
    "        # tax_amount\n",
    "        (tax_amount, exception) = dv_fc(rule_name = 'check_len', args_name = 'tax_amount', args = tax_amount, msg = exception, paras=(12, 5))\n",
    "\n",
    "        # total_amount\n",
    "        (total_amount, exception) = dv_fc(rule_name = 'check_len', args_name = 'total_amount', args = total_amount, msg = exception, paras=(12, 5))\n",
    "\n",
    "        # agency_code\n",
    "        (agency_code, exception) = dv_fc(rule_name = 'check_len', args_name = 'agency_code', args = agency_code, msg = exception, paras=(10, 0))\n",
    "\n",
    "        # source\n",
    "        (source, exception) = dv_fc(rule_name = 'check_len', args_name = 'source', args = source, msg = exception, paras=(10, 0))\n",
    "\n",
    "        # coupon_used\n",
    "        (coupon_used, exception) = dv_fc(rule_name = 'check_len', args_name = 'coupon_used', args = coupon_used, msg = exception, paras=(10, 0))\n",
    "\n",
    "\n",
    "    return [exception, original_fare,exchange_rate,fare_amount,tax_amount,total_amount, agency_code, source, coupon_used]\n",
    "\n",
    "bos_df_csv = etl_fc(bos_df_csv, fc, input, output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sc = SparkContext.getOrCreate();\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "my_conn_options = {\n",
    "    \"dbtable\": \"flextravel.fx_trans_file\",\n",
    "    \"database\": \"fts_cp_uat\",\n",
    "    \"url\": \"jdbc:postgresql://flextravel-uat-serverless-pg.chzjoncadzav.ca-central-1.rds.amazonaws.com:5432/fts_cp_uat\",\n",
    "    \"customJdbcDriverS3Path\":\"s3://flex-data-uat-canda-central/DEV/postgresql-42.6.0.jar\",\n",
    "    \"customJdbcDriverClassName\":\"org.postgresql.Driver\",\n",
    "    \"user\":\"flexuatuser\",\n",
    "    \"password\":\"flexnewyearpwd@2022\"\n",
    "}\n",
    "\n",
    "my_conn_options1 = {\n",
    "    \"dbtable\": \"flextravel.general_info\",\n",
    "    \"database\": \"fts_cp_uat\",\n",
    "    \"url\": \"jdbc:postgresql://flextravel-uat-serverless-pg.chzjoncadzav.ca-central-1.rds.amazonaws.com:5432/fts_cp_uat\",\n",
    "    \"customJdbcDriverS3Path\":\"s3://flex-data-uat-canda-central/DEV/postgresql-42.6.0.jar\",\n",
    "    \"customJdbcDriverClassName\":\"org.postgresql.Driver\",\n",
    "    \"user\":\"flexuatuser\",\n",
    "    \"password\":\"flexnewyearpwd@2022\"\n",
    "}\n",
    "\n",
    "my_conn_options2 = {\n",
    "    \"dbtable\": \"public.fx_trans_bre_interim_bos_sprint2_test\",\n",
    "    \"database\": \"fts_cp_uat\",\n",
    "    \"url\": \"jdbc:postgresql://flextravel-uat-serverless-pg.chzjoncadzav.ca-central-1.rds.amazonaws.com:5432/fts_cp_uat\",\n",
    "    \"customJdbcDriverS3Path\":\"s3://flex-data-uat-canda-central/DEV/postgresql-42.6.0.jar\",\n",
    "    \"customJdbcDriverClassName\":\"org.postgresql.Driver\",\n",
    "    \"user\":\"flexuatuser\",\n",
    "    \"password\":\"flexnewyearpwd@2022\"\n",
    "}\n",
    "\n",
    "df = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"postgresql\",\n",
    "    connection_options=my_conn_options,\n",
    "    transformation_ctx=\"df\",\n",
    ")\n",
    "\n",
    "df_GI = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"postgresql\",\n",
    "    connection_options=my_conn_options1,\n",
    "    transformation_ctx=\"df\",\n",
    ")\n",
    "# GI_df = df_GI.toDF().select(\"id\",\"tids_code\")\n",
    "\n",
    "# # bos_df_csv=  bos_df_csv.withColumn( \"Orginal_currency\",F.when(length(col(\"Org_currency\"))>5,'').otherwise(bos_df_csv.Org_currency))\n",
    "\n",
    "# bos_df_final = bos_df_csv.select('agency_code', 'ticket_number', 'issue_date', 'pnr', 'tour_code', 'passenger_name', 'coupon_used', 'original_fare', 'fare_amount', 'exchange_rate', 'commission_amount', 'original_currency', 'tax_amount', 'total_amount', 'commission', 'org_ticket_no', 'tax', 'legs', 'fare_construction', 'payment','refund_legs','exception')\n",
    "\n",
    "# Bos_df_GI= bos_df_final.join(GI_df,\n",
    "#                bos_df_csv.agency_code == GI_df.tids_code,\n",
    "#                \"left\")\n",
    "\n",
    "\n",
    "# Bos_df_GI=Bos_df_GI.withColumnRenamed(\"id\",\"agent_id\")\n",
    "\n",
    "#Bos_df_GI.show(2, truncate=False)\n",
    "\n",
    "# df1 = df.toDF().select(\"id\", \"supplier_id\",\"sup_srv_map_id\",\"file_name\").where(df[\"file_name\"] =='06.07_DCALLRECORDSAM')\n",
    "# #df2=df1.withColumnRenamed(\"sup_srv_map_id\",\"col0\").withColumnRenamed(\"file_name\",\"col1\")\n",
    "# # df1 = df.filter(f=lambda x: x[\"sup_srv_map_id\"] in [65])\n",
    "\n",
    "# df1 = df.toDF()[\"id\", \"supplier_id\",\"sup_srv_map_id\",\"file_name\"]\n",
    "# df1 = df1[df1[\"file_name\"] =='06.07_DCALLRECORDSAM']\n",
    "\n",
    "\n",
    "#df1.show()\n",
    "\n",
    "# Bos_df_file= Bos_df_GI.join(df1,\n",
    "#                bos_df_csv.tax_on_commission == df1.file_name,\n",
    "#                \"left\")\n",
    "\n",
    "# Bos_df_file.show()\n",
    "\n",
    "# FTS - 2598 [\"Tax_Amt\"].cast(IntegerType()) -> DoubleType, [\"Total_amt\"].cast(IntegerType()) -> DoubleType\n",
    "Bos_df_file = bos_df_csv\n",
    "Bos_df_file= Bos_df_file.withColumn(\"original_fare\", Bos_df_file[\"original_fare\"].cast(DoubleType())).withColumn(\"fare_amount\", Bos_df_file[\"fare_amount\"].cast(DoubleType())).withColumn(\"tax_amount\", Bos_df_file[\"tax_amount\"].cast(DoubleType()))\\\n",
    "            .withColumn(\"total_amount\", Bos_df_file[\"total_amount\"].cast(DoubleType())).withColumn(\"exchange_rate\", Bos_df_file[\"exchange_rate\"].cast(DoubleType()))\n",
    "\n",
    "Bos_df_file= Bos_df_file.withColumn(\"commission_amount\", Bos_df_file[\"commission_amount\"].cast(DoubleType())).withColumn(\"issue_date\", Bos_df_file[\"issue_date\"].cast(TimestampType()))\n",
    "\n",
    "Bos_df_write = Bos_df_file.select('transaction_date', 'source', 'booking_channel', 'version_no', 'currency_code', 'ticket_type','country_code', 'passenger_count','agency_code', 'ticket_number', 'issue_date', 'pnr', 'tour_code', 'passenger_name', 'coupon_used', 'original_fare', 'fare_amount', 'exchange_rate', 'commission_amount', 'original_currency', 'tax_amount', 'total_amount', 'commission', 'org_ticket_no', 'tax', 'legs', 'fare_construction', 'payment','refund_legs','exception', 'file_path','pos')\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Bos_df_write_dyn=DynamicFrame.fromDF(Bos_df_write,glueContext,'Bos_df_write_dyn')\n",
    "\n",
    "df3=glueContext.write_dynamic_frame_from_options(\n",
    "    frame=Bos_df_write_dyn,\n",
    "    connection_type=\"postgresql\",\n",
    "    connection_options=my_conn_options2,\n",
    "    transformation_ctx=\"dynamic_frame\"\n",
    ")\n",
    "#df3.show()"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "PythonException: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/util.py\", line 87, in wrapper\n    return f(*args, **kwargs)\n  File \"<stdin>\", line 30, in fc_Fare\nUnboundLocalError: local variable 'fare' referenced before assignment\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# # mini batch test\n",
    "# from pyspark.sql import SQLContext\n",
    "# sqlContest = SQLContext(sc)\n",
    "\n",
    "# i = 0\n",
    "# k = 19000\n",
    "# j = 100\n",
    "\n",
    "# while (i+j < k):\n",
    "#     print(i)\n",
    "\n",
    "#     try:\n",
    "#         pdk = Bos_df_write.toPandas()\n",
    "#         pd1 = pdk.loc[i:i+j][:].fillna(0)\n",
    "#         spark_df = sqlContest.createDataFrame(pd1)\n",
    "\n",
    "#         Bos_df_write_dyn1=DynamicFrame.fromDF(spark_df,glueContext,'Bos_df_write_dyn')\n",
    "\n",
    "#         df3=glueContext.write_dynamic_frame_from_options(\n",
    "#             frame=Bos_df_write_dyn1,\n",
    "#             connection_type=\"postgresql\",\n",
    "#             connection_options=my_conn_options2,\n",
    "#             transformation_ctx=\"dynamic_frame\"\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#     finally:\n",
    "#         i = i + j\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.3 \nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::251954244960:role/aws-glue-role\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 94e43814-4481-46a2-bd42-9a375b67a951\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.3\n--enable-glue-datacatalog true\nWaiting for session 94e43814-4481-46a2-bd42-9a375b67a951 to get into ready status...\nSession 94e43814-4481-46a2-bd42-9a375b67a951 has been created.\n\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}